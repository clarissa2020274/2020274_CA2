{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0e1860",
   "metadata": {},
   "source": [
    "### MSc Data Analytics \n",
    "\n",
    "##### CA2 - Integrated Assignment sem2\n",
    "\n",
    "#### 2020274 - Clarissa Cardoso\n",
    "\n",
    "\n",
    "##### Github repository: https://github.com/clarissa2020274/2020274_CA2_sem2.git\n",
    "\n",
    "\n",
    "This Notebook contains experimental features for CA2. The goal of this project is to combine language processing techniques and a time series forecasting to predict the average sentiment of tweets for a certein period of time after the apropriate data cleaning and processing tecniques are applied.\n",
    "\n",
    "F0r a better understanding the project will be divided into the following sections:\n",
    "\n",
    "- First section consists of importing dataset from hdfs, and an attemp to evaluate their performance and usability, helping to select the most suitable dataset for the analysis, using spark's distribuited system for processing.\n",
    "\n",
    "- Second section will focus on data cleaning and preprocessing the dataset.\n",
    "\n",
    "- Third section focus on deeper EDA features and Natural Language Processing to undertand the dataset better prior to modeling and extract the sentiment from tweets given, such as removing stopwords, special characters etc..\n",
    "\n",
    "\n",
    "Sections 4 and 5 will be deployed on a separate Jupyter Notebook because of compuational issues within Ubuntu Os.\n",
    "\n",
    "When attempting to clear space in local disk, a few libraries inicially used for the project were not able to be updated when the system kept freezing for lack of space. The solution found was to reinstall all necessary packages through a virtual enviroment where the new Notebook (2020274_MScDA_CA2_Clarissa_SecondNotebook.ipynb) will be used for experimentation of timeseries contruction. Once the data prepocessing and cleaning was complete, a new file was stored in HDFS and local drive to enable access from multiple sources. \n",
    "\n",
    "- Section four centers on creating the time-series model and selection of apropriate parametrers and hyperparameters to run it.\n",
    "\n",
    "- Fifth section relies on training the model and validating/reacessing features that can be modified for better performance and compare model's results. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7da644",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "\n",
    "The goal of this project is to perform an analysis of the given dataset containing several tweets while experimenting with different databases to store data as well as creating a  time series forecast of the sentiment of the dataset. \n",
    "\n",
    "For the initial experimentation, after instalation of different noSQL databases as seen in class tutorials, I have decided to start with Hbase. One of the reasons why this was the first database used for the project is that it is built on top of HDFS as a part of Hadoop environment and provides a faster lookup on files while displaying lower latency for queries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f9e8f",
   "metadata": {},
   "source": [
    "### Libraries required for project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f79db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3ac7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries to deploy pyspark functions\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import count # Funcion to get the \"size\" of the data.\n",
    "from pyspark.sql.functions import when # When function.\n",
    "from pyspark.sql.functions import col # Function column.\n",
    "from pyspark.sql.functions import mean, min, max, stddev # Imports function for statistical features. \n",
    "from pyspark.sql import functions as F # Data processing framework.\n",
    "from pyspark.sql.functions import size, split # Imports function size and split.\n",
    "from pyspark.ml.feature import Tokenizer # Importing Tokenizer.\n",
    "from pyspark.sql.functions import regexp_replace # Remove / Replace function.\n",
    "from pyspark.sql.types import StructField, StructType # Importing features for Schema.\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType # Tools to create the schema.\n",
    "from pyspark.sql.functions import udf # Imports function UDF (user defined functions).\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import max as max_\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "import numpy as np # for numerical operations.\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "%matplotlib inline \n",
    "\n",
    "import nltk # natural language toolkit for language processing\n",
    "#nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose # Imports seasonal decompose for time-series analysis.\n",
    "from statsmodels.tsa.stattools import adfuller # Statistical approach for stationarity.\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Auto-correlation plots for time-series analysis.\n",
    "from statsmodels.tsa.ar_model import AutoReg # Imports Auto Regressive\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # Imports metrics for evaluation.\n",
    "from scipy import stats # Stats is for Shapiro test (normality).\n",
    "from statsmodels.tsa.arima.model import ARIMA # ARIMA time series model.\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX# Imports SARIMA\n",
    "import itertools # For hyper-parameter tunning.\n",
    "import pmdarima as pm # Imports pmdarima to find optimal order values.\n",
    "from pmdarima import auto_arima # Imports auto_arima for optimal parameters.\n",
    "\n",
    "\n",
    "\n",
    "import warnings # Ignore warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f31383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b712d94",
   "metadata": {},
   "source": [
    "#### Importing dataset from HDFS\n",
    "\n",
    "My initial idea, once the dataset given was succesfuly stored in HDFS directory alocated for the CA development (\"CA2/ProjectTweets.csv\"), was to import it straighaway to a noSQL database and perform initial queries inside the HBase enviroment/shell to verify functionality.\n",
    "\n",
    "However my VM had continuous crashes during this process, and the HMaster node managed by Zookeeper kept showing slower times for initializing the commands. After a few seconds the Zookeeper Connection with HDFS and Hase nodes was lost and it was taking me a longer time span to find an alternative. Since the csv file was already in hadoop, I decided to first import from HDFS and the perform some initial cleaning and EDA using Spark framework to process the data to then store the cleneaded data back to HBAse through a connector between Pyspark and the database.\n",
    "\n",
    "\n",
    "- HDFS (Hadoop Distributed File System) is the primary storage system used by Hadoop applications. This open source framework works by rapidly transferring data between nodes. It's often used by companies who need to handle and store big data. <https://www.databricks.com/glossary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc6424",
   "metadata": {},
   "source": [
    "## Import modules, create Spark Session and read file into dataframe\n",
    "\n",
    "First step is to perform some basic exploratory data analysis to get a sense of the data. \n",
    "\n",
    "#### Check the first few rows of the dataset with .show()\n",
    "\n",
    "File was imported with a header marked as 'false' so pyspark will input labels insted of using the first row. This makes room to rename the labels in coming steps. set up schema as true so pyspark utilises the same scema present in the original file, without overlapping the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0863cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession including Legacy for timestamp\n",
    "spark = SparkSession.builder.appName(\"Test Tweets\")\\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"com.johnsnowlabs.nlp.serialization.SparkNLPKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the file path in HDFS\n",
    "file_path = \"hdfs:///user/hduser/CA2/ProjectTweets.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "tweets_test = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame (optional)\n",
    "tweets_test.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e0566",
   "metadata": {},
   "source": [
    "#### Checking the schema of the dataset\n",
    "\n",
    "From this function we see most of the data is composed by strings, which makes sense, since we are working with mostly text. However, on the third column, with the dates of each tweet, we must have a datetime datatype in order to perform the timeseries analysis on further stages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a294330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5889696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|summary|               _c0|                 _c1|                 _c2|     _c3|                 _c4|                 _c5|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|1.9988175522956276E9|                null|    null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|1.9357607362267256E8|                null|    null|5.162733218454889E10|                null|\n",
      "|    min|                 0|          1467810369|Fri Apr 17 20:30:...|NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|           1599999|          2329205794|Wed May 27 07:27:...|NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get summary statistics\n",
    "tweets_test.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a2833",
   "metadata": {},
   "source": [
    "### Part I : Data Pre-Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59afc2",
   "metadata": {},
   "source": [
    "\n",
    "Rename cols and drop c1\n",
    "\n",
    "Convert the date column to a timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508dd527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|index|                date|query_flag|           user|                text|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|    0|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|    5|Mon Apr 06 22:20:...|  NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|Mon Apr 06 22:20:...|  NO_QUERY|        mybirch|         Need a hug |\n",
      "|    7|Mon Apr 06 22:20:...|  NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|Mon Apr 06 22:20:...|  NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|Mon Apr 06 22:20:...|  NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|   10|Mon Apr 06 22:20:...|  NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|   11|Mon Apr 06 22:20:...|  NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|   12|Mon Apr 06 22:20:...|  NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|   13|Mon Apr 06 22:20:...|  NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|Mon Apr 06 22:20:...|  NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|   15|Mon Apr 06 22:20:...|  NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|Mon Apr 06 22:20:...|  NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|   17|Mon Apr 06 22:20:...|  NO_QUERY|       armotley|about to file taxes |\n",
      "|   18|Mon Apr 06 22:20:...|  NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|Mon Apr 06 22:20:...|  NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.drop(\"_c1\") \\\n",
    "           .withColumnRenamed(\"_c0\", \"index\") \\\n",
    "           .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "           .withColumnRenamed(\"_c3\", \"query_flag\") \\\n",
    "           .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "           .withColumnRenamed(\"_c5\", \"text\") \\\n",
    "\n",
    "tweets_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607de02",
   "metadata": {},
   "source": [
    "View a sample of the 'date' column, using the sample() function to double check the timezone used before conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e018e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                date|\n",
      "+--------------------+\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:25:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:31:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see 10% of date row to see correct timezone before converting\n",
    "tweets_test.select(\"date\").sample(False, 0.1, seed=42).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093c5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 06 22:19:45 PDT 2009\n",
      "Mon Apr 06 22:19:49 PDT 2009\n",
      "Mon Apr 06 22:19:53 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:20:00 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:05 PDT 2009\n",
      "Mon Apr 06 22:20:09 PDT 2009\n"
     ]
    }
   ],
   "source": [
    "# limit to 10 rows of date col.\n",
    "sample_date_values = tweets_test.select(\"date\").limit(10).collect()\n",
    "for row in sample_date_values:\n",
    "    print(row.date)\n",
    "\n",
    "    #with this we can confirm the PDT - Pacific Day Time for apropriate conversion to timestamp.\n",
    "    # this may influence further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d861e2",
   "metadata": {},
   "source": [
    "it's important to account for the PDT timezone used. When converting to datetime, the new schema was in the apropriate datatypes, however when i tried to sample the 'date' rows again i got an error as seen below: \n",
    "\n",
    "> <font color='red'> <b>Py4JJavaError:</b> An error occurred while calling o100.showString.\n",
    ": org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM dd HH:mm:ss z yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html </font> \n",
    "\n",
    "So according to Apache Spark documentation, I added a date parsing from java with SimpleDateFormat class to allow customization of the date format of the strings. For that the timezone needs to be specified to avoid any discrepancies. In this case, PDT is UTC-7 which is represented by 'z' in the Apache datetime patterns doc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b953588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.withColumn(\"date\", to_timestamp(tweets_test.date, \"EEE MMM dd HH:mm:ss z yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5df066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715351a",
   "metadata": {},
   "source": [
    "#### Checking for missing values and shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e25325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----+----+\n",
      "|index|date|query_flag|user|text|\n",
      "+-----+----+----------+----+----+\n",
      "|    0|   0|         0|   0|   0|\n",
      "+-----+----+----------+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "# Check for missing values in each column\n",
    "tweets_test.select([count(when(col(c).isNull(), c)).alias(c) for c in tweets_test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e0e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 4) / 4]\r",
      "\r",
      "[Stage 12:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1600000\n",
      "Number of Columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the sahpe of the dataset\n",
    "num_rows = tweets_test.count()\n",
    "num_cols = len(tweets_test.columns)\n",
    "\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feb13d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa663cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|index|               date|query_flag|           user|                text|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|    0|2009-04-07 05:19:45|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|2009-04-07 05:19:49|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|2009-04-07 05:19:53|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|2009-04-07 05:19:57|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|2009-04-07 05:19:57|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test.dropna()  # Drop rows containing NaN values for simplicity\n",
    "tweets_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbcb54",
   "metadata": {},
   "source": [
    "#### Pyspark has some inbuilt functions for starting tne text processing, such as lowercasing, removing special characters and stopwords. \n",
    "\n",
    "The following text data preprocessing steps using PySpark functions are:\n",
    "\n",
    "Lowercasing: We use the lower() function to convert all text to lowercase.\n",
    "\n",
    "Removing Special Characters: We use regexp_replace() to remove any characters that are not alphanumeric or whitespace.\n",
    "\n",
    "\n",
    "The resulting tweets_test will display the preprocessed text in the 'text' column.\n",
    "\n",
    "PySpark doesn't have built-in support for stemming or lemmatization. To implement these kind of techniques, external libraries such as nltk can be implemented/imported. \n",
    "<b>NLTK<b/> provides a wide range of tools and resources for working with human language data, and it can complement Spark's capabilities in certain scenarios. \n",
    "\n",
    "However, when i tried to import the nltk functions to my vm, i encountered a series of incompatibility issues. Even after creating a virtual environment i was not able to install the library, and the same happened when i tried to install <b>Sparknlp<b/>, which is the language processing tool whitin the Spark enviroment considered to be the state of the art for a number of functionalities in the NLP area. \n",
    "\n",
    "There was also an attempt, as suggested by SparkNLP documentaroin, to inicialize pyspark with the additional packages for language processing but when i tried to import it back to the notebook it would not find the module installed, even when i apply the same command (pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4) to the virtual env the packages were installed. A third nlp library was also atempted to import via pip: <b>textblob<b/>\n",
    "    \n",
    "#error: externally-managed-environment (#error: externally-managed-environment)\n",
    "\n",
    "With the command found in stackoverflow (sudo apt install python3-nltk) available at https://askubuntu.com/questions/996185/how-can-i-install-nltk-for-python-3\n",
    "\n",
    "i was able to go back to my original choice of applying nltk tools for extracting the sentiment of the tweets given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1995f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4 using virtual enviroment 'myenv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690a9b3",
   "metadata": {},
   "source": [
    "### Removing noise from data with Spark's functions: \n",
    "- Special characters\n",
    "- Transform all text data to lower case letters\n",
    "- Remove numbers, duplicate characters and aditional punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb543ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|my whole body feels itchy and like its on fire                                                             |\n",
      "| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "| not the whole crew                                                                                        |\n",
      "|need a hug                                                                                                 |\n",
      "| hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you                           |\n",
      "| nope they didnt have it                                                                                   |\n",
      "| que me muera                                                                                              |\n",
      "|spring break in plain city11 its snowing                                                                   |\n",
      "|i just repierced my ears                                                                                   |\n",
      "| i couldnt bear to watch it  and i thought the ua loss was embarrassing                                    |\n",
      "| it it counts idk why i did either you never talk to me anymore                                            |\n",
      "| i wouldve been the first but i didnt have a gun11not really though zac snyders just a doucheclown         |\n",
      "| i wish i got to watch it with you i miss you and11how was the premiere                                    |\n",
      "|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                |\n",
      "|about to file taxes                                                                                        |\n",
      "| ahh ive always wanted to see rent  love the soundtrack                                                    |\n",
      "| oh dear were you drinking out of the forgotten table drinks                                               |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"http(s)?://[^\\\\s]+\", \"\"))\n",
    "\n",
    "# Remove HTML tags\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# Remove mentions (i.e., @username)\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"@\\\\w+\", \"\"))\n",
    "\n",
    "# Convert to lowercase\n",
    "tweets_test = tweets_test.withColumn('text', lower(tweets_test['text']))\n",
    "\n",
    "# Remove numbers from the \"text\" column\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace(tweets_test['text'], r'\\d+', ''))\n",
    "\n",
    "# Reduce excessive characters (more than two of the same in a row)\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace('text', r'(.)\\1{2,}', r'\\1\\1'))\n",
    "\n",
    "# Remove punctuation\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace(tweets_test['text'], r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "tweets_test.select(\"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef984308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240940f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'flag' and 'user' columns\n",
    "tweets_test = tweets_test.drop('query_flag', 'user')\n",
    "\n",
    "# Show the result\n",
    "tweets_test.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043d253",
   "metadata": {},
   "source": [
    "### Write Cleaned dataset in a new csv back to hdfs. \n",
    "\n",
    "with code adapted from SparkByExamples[], let's store the cleaned tweets to a new csv file back to hdfs. \n",
    "This will be later on imported to mysql for testing different databases. the code will be left eith comments just to protect the vm, and avoid overwriting the files in case the kernel is restarted. \n",
    "\n",
    "[] https://sparkbyexamples.com/spark/spark-write-dataframe-to-csv-file/#:~:text=In%20Spark%2C%20you%20can%20save,any%20Spark%20supported%20file%20systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9870d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path in HDFS where  to save the new DataFrame\n",
    "#new_file_path = \"hdfs:///user/hduser/CA2/CleanTweets.csv\"\n",
    "\n",
    "# Save the DataFrame to HDFS as a CSV file\n",
    "#tweets_test.write.csv(new_file_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59731c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path in HDFS where to store the clean DataFrame\n",
    "#output_directory = \"hdfs:///user/hduser/CA2\"\n",
    "\n",
    "# Save the DataFrames to HDFS as CSV files\n",
    "#tweets_test.write.csv(f\"{output_directory}/CleanTweets\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b544c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "|5    |2009-04-07 05:20:00| not the whole crew                                                                                        |\n",
      "|6    |2009-04-07 05:20:03|need a hug                                                                                                 |\n",
      "|7    |2009-04-07 05:20:03| hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you                           |\n",
      "|8    |2009-04-07 05:20:05| nope they didnt have it                                                                                   |\n",
      "|9    |2009-04-07 05:20:09| que me muera                                                                                              |\n",
      "|10   |2009-04-07 05:20:16|spring break in plain city11 its snowing                                                                   |\n",
      "|11   |2009-04-07 05:20:17|i just repierced my ears                                                                                   |\n",
      "|12   |2009-04-07 05:20:19| i couldnt bear to watch it  and i thought the ua loss was embarrassing                                    |\n",
      "|13   |2009-04-07 05:20:19| it it counts idk why i did either you never talk to me anymore                                            |\n",
      "|14   |2009-04-07 05:20:20| i wouldve been the first but i didnt have a gun11not really though zac snyders just a doucheclown         |\n",
      "|15   |2009-04-07 05:20:20| i wish i got to watch it with you i miss you and11how was the premiere                                    |\n",
      "|16   |2009-04-07 05:20:22|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                |\n",
      "|17   |2009-04-07 05:20:25|about to file taxes                                                                                        |\n",
      "|18   |2009-04-07 05:20:31| ahh ive always wanted to see rent  love the soundtrack                                                    |\n",
      "|19   |2009-04-07 05:20:34| oh dear were you drinking out of the forgotten table drinks                                               |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "tweets_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69981b80",
   "metadata": {},
   "source": [
    "## Convert to Pandas dataframe for applying nltk:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd7eb9",
   "metadata": {},
   "source": [
    "Among the main differences between Pandas & PySpark, is that operations on Pyspark run faster than Pandas due to its distributed nature and parallel execution on multiple cores and machines. This means pandas run operations on a single node whereas PySpark runs on multiple machines. When working on a Machine Learning applications manipulating larger datasets, PySpark processes operations many times faster than pandas. In this project case, however, when i tried to implement the Spark-nlp pipeline, and was not able to preoceed due to internal errors, the second option was to use the nltk library to get the sentiment from the tweets. \n",
    "\n",
    "According to both documentations (Spark and nltk) they are compatible and be used together but while I was applyting the nktk to my spark session, I kept having a series od erros inside the Linux environment, so I converted my dataset back to a pandas dataframe and was able to continue the analysis. \n",
    "\n",
    "It was the trade-off encoutered, to slow down performance but at least get an output to present at the end of the project. Another trade-off involved is betweeen performance and cost when it comes to scaling a cluster in spark, but for the time constraints involved it was left in the original distribution of 4 separate nodes in the vm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fd3f0",
   "metadata": {},
   "source": [
    "In terms of sentimental analysis, there is a wide range of libraries available for perfoming the models. \n",
    "For this project, I tried to install 3 different libraries: vaderSentiment, Sparknlp and nltk, and faced a few errors in the process, but was able to install only the ladder: nltk which is a non-spark nlp library that require pandas usewr defined functions to work. \n",
    "Among the literature found for this project, the majority of sources agree that when working with Spark-native distributed NLP projects, the SparkNLP library is the most popular and comprehensive library, for its open-source caracteristics and functionality. The ability to create a simple pipeline to extract sentiment and perfomr advanced analysis are the reason its considerted to be the \"State of the art for language processing\". \n",
    "\n",
    "The VADER sentiment analyzer is a simple rule-based model that works well for social media text. For a more advanced sentiment analysis, there are also other machine learning-based approaches, such as using pre-trained models like BERT or spaCy, that can offer a more detailed modelling.\n",
    "\n",
    "Databricks community, available at:  https://community.databricks.com/t5/machine-learning/what-are-best-nlp-libraries-to-use-with-spark/td-p/24033#:~:text=You%20can%20also%20use%20spacy,them%20to%20data%20with%20Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfa7bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ConvertToPandas\").getOrCreate()\n",
    "\n",
    "# Assuming 'tweets_test' is your Spark DataFrame\n",
    "# For example, if you have a DataFrame named 'tweets_test', you can use the following code:\n",
    "\n",
    "# Convert 'date' column to timestamp type\n",
    "tweets_test = tweets_test.withColumn('date', col('date').cast('timestamp'))\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = tweets_test.select('*').toPandas()\n",
    "\n",
    "# Now 'pandas_df' is a Pandas DataFrame with the 'date' column in datetime format\n",
    "# You can proceed with your analysis using Pandas and other Python libraries\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d19876b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'setCallSite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m \u001b[43mtweets_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Now 'pandas_df' is a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#  apply NLTK or other Python libraries on 'pandas_df' as needed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# using the same code there was a datatype error between the original kernel and the venv kernel used for the copy\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#so to modify the code, the conversion to an apropriate dtype was done as folowing:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:692\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSCCallSiteSync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcss\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/traceback_utils.py:72\u001b[0m, in \u001b[0;36mSCCallSiteSync.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_site)\n\u001b[1;32m     73\u001b[0m     SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setCallSite'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = tweets_test.toPandas()\n",
    "\n",
    "# Now 'pandas_df' is a Pandas DataFrame\n",
    "#  apply NLTK or other Python libraries on 'pandas_df' as needed\n",
    "\n",
    "# using the same code there was a datatype error between the original kernel and the venv kernel used for the copy\n",
    "\n",
    "#so to modify the code, the conversion to an apropriate dtype was done as folowing:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "#pandas_df = tweets_test.withColumn('date', col('date').cast('timestamp')).toPandas()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "#pandas_df = tweets_test.withColumn('date', col('date').cast('timestamp').alias('date')).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ce2294b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43mpandas_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pandas_df\u001b[38;5;241m.\u001b[39minfo())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pandas_df' is not defined"
     ]
    }
   ],
   "source": [
    "display(pandas_df.head())\n",
    "print(pandas_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6d38a",
   "metadata": {},
   "source": [
    "### Tokenization with Spark functions\n",
    "\n",
    "this session was left commented since the nltk library was the main choice to perform these function, but the spark tokenization could also have been implemented to compose the pipeline for sentiment analisis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4213e0e7",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tweets_df = tokenizer.transform(tweets_test)\n",
    "\n",
    "# Show the result\n",
    "tweets_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7ef27",
   "metadata": {},
   "source": [
    "### Text Preprocessing for sentiment extraction using NLTK library\n",
    "\n",
    "Text preprocessing is a crucial step in sentiment analysis, facilitating the cleaning and normalization of text data for effective analysis. \n",
    "Common techniques include:\n",
    "\n",
    "- Tokenization: which breaks down text into individual words or tokens,\n",
    "- Stop word removal:  to eliminate irrelevant words, \n",
    "- Stemming and lemmatization for reducing words to their root forms. \n",
    "\n",
    "In this project, tokenization of the text column is done with NLTK's word_tokenize function, while stop words can be removed using NLTK's built-in list. Stemming reduces words to base forms by removing suffixes, and lemmatization does the same based on the part of speech. These techniques enhance sentiment analysis accuracy. The bag of words (BoW) model, a natural language processing technique, represents text as numerical features, allowing for analysis with machine learning algorithms.\n",
    "\n",
    "It was observed a longer time to process this function, so to measure the accurate execution time, a pandas function was added to the preprocess_text function created below:\n",
    "\n",
    "Original Source: DataCamp NLTK Tutorial (Accessed on 17/11/2023, available at:https://www.datacamp.com/tutorial/text-analytics-beginners-nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f384f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Apply the function to pandas_df and measure the time\n",
    "start_time = time.time()\n",
    "pandas_df['text'] = pandas_df['text'].apply(preprocess_text)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the total execution time\n",
    "print(f\"Total execution time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "#code adapted from Datacamp NLTK Sentiment Analysis Tutorial for Beginners- \n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f326e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29ea3905",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# apply the function to pandas_df\n",
    "\n",
    "#pandas_df['text'] = pandas_df['text'].apply(preprocess_text)\n",
    "#pandas_df.head()\n",
    "\n",
    "#code adapted from Datacamp NLTK Sentiment Analysis Tutorial for Beginners- \n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642a84e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ea5f26",
   "metadata": {},
   "source": [
    "#### NLTK sentiment analyzer\n",
    "The NLTK sentiment analyzer below returns a score between -1 and +1. We have used a cut-off threshold of 0 in the get_sentiment function. Anything above 0 is classified as 1 (meaning positive). \n",
    "\n",
    "In datacamp's original code and dataset, they had the actual labels available to perform the confusion matrix and get the classification report to evaluate the performance of the method. In the case of the dataset provided for this project, we only have access to the predicted labels obtained with nltk sentiment analyzer, so when i tried to get a classification report using a similar code, the results seemlengly perfect (1.0 accuracy?) are not really realiable since there are no actual data to compare them with. The classification report has 'perfect' performance because its using only the predicted values for sentiment, instead real data. \n",
    "\n",
    "When calculated the Internal Consistency to check the percentage of consistent predictions, the reult of 44.89% indicates that almost half of the predicted sentiment labels are consistent with each other. However, it's important to note that without ground truth labels for comparison, internal consistency alone doesn't provide a complete picture of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# create get_sentiment function\n",
    "def get_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0\n",
    "    return sentiment\n",
    "\n",
    "# apply get_sentiment function\n",
    "\n",
    "pandas_df['sentiment'] = pandas_df['text'].apply(get_sentiment)\n",
    "\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dist of sentiment:\n",
    "\n",
    "pandas_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d387da6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(pandas_df['sentiment'], pandas_df['sentiment'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, cross_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn\n",
    "#error: externally-managed-environment\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae232bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# Considering 'sentiment' is the predicted sentiment column\n",
    "predicted_labels = pandas_df['sentiment']\n",
    "\n",
    "# Evaluate internal consistency since there are not true lables, only the predicted ones \n",
    "\n",
    "# First, if 'sentiment' is binary (0 and 1), you can check the distribution\n",
    "print(\"Distribution of Predicted Sentiment:\")\n",
    "print(predicted_labels.value_counts())\n",
    "\n",
    "# You can use other evaluation metrics as well\n",
    "# For example, internal consistency (percentage of consistent predictions)\n",
    "internal_consistency = (predicted_labels == 0).mean()  # Adjust as needed\n",
    "print(f\"\\nInternal Consistency: {internal_consistency * 100:.2f}%\")\n",
    "\n",
    "# If 'sentiment' is binary, you can also check the confusion matrix\n",
    "# and classification report\n",
    "conf_matrix = confusion_matrix(predicted_labels, predicted_labels)\n",
    "class_report = classification_report(predicted_labels, predicted_labels)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c40f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73417998",
   "metadata": {},
   "source": [
    "### Adding a third label for display neutrality between the polarised thresholds\n",
    "\n",
    "With the same function for sentimental analyzer, but adding another category so the output will display positive, negative and neutral sentiments for a more comprehensive dataset. The compound score returned by the SentimentIntensityAnalyzer in NLTK is a single numerical score that represents the overall sentiment of the text. It ranges from -1 (most negative) to 1 (most positive), with values near 0 indicating neutrality.\n",
    "\n",
    "- If the compound score is greater than pos_threshold, it's classified as \"positive.\"\n",
    "- If the compound score is less than neg_threshold, it's classified as \"negative.\"\n",
    "- If the compound score is between -neu_threshold and neu_threshold, it's classified as \"neutral.\"\n",
    "- If none of the above conditions are met, the sentiment is set to None.\n",
    "\n",
    "In this case it was tested with only one treshold due to computational resourses inside ubuntu os. However, since sentimental analysis can be rather subjective science, documentation suggests a more conservative aproach (leaning towards classification of higher positive scores) would be to increase the positive treshold values, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create get_sentiment function adding a third category = POS, NEG, NEU\n",
    "def get_sentiment(text, pos_threshold=0.1, neg_threshold=-0.1, neu_threshold=0.1):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Classify as positive, negative, or neutral based on thresholds\n",
    "    if scores['compound'] > pos_threshold:\n",
    "        sentiment = 'positive'\n",
    "    elif scores['compound'] < neg_threshold:\n",
    "        sentiment = 'negative'\n",
    "    elif abs(scores['compound']) <= neu_threshold:\n",
    "        sentiment = 'neutral'\n",
    "    else:\n",
    "        sentiment = None\n",
    "    \n",
    "    return scores['compound'], sentiment\n",
    "\n",
    "# Apply get_sentiment function and adding two new columns with score and label\n",
    "#Measure execution time \n",
    "start_time = time.time()\n",
    "pandas_df['sentiment_score'], pandas_df['label'] = zip(*pandas_df['text'].apply(get_sentiment))\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total execution time\n",
    "print(f\"Total execution time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152e6c3",
   "metadata": {},
   "source": [
    "### Store the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd490409",
   "metadata": {},
   "source": [
    "### Visialisation of the sentimnent distribuition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of labels distribuition\n",
    "cm = confusion_matrix(pandas_df['label'], pandas_df['label'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.xlabel('Predicted Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dist of sentiment labels:\n",
    "\n",
    "pandas_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dist of sentiment:\n",
    "\n",
    "pandas_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820ca09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# You can use the 'label' column for sentiment categories\n",
    "\n",
    "# Plotting the distribution histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "pandas_df['label'].value_counts().plot(kind='bar', color=['red', 'green', 'blue'], alpha=0.7)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Distribution of Sentiment Categories')\n",
    "plt.xlabel('Sentiment Categories')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# selecting 'sentiment' is the predicted sentiment column\n",
    "predicted_labels = pandas_df['sentiment']\n",
    "\n",
    "# Evaluate internal consistency since there are not true lables, only the predicted ones \n",
    "\n",
    "# First, if 'sentiment' is binary (0 and 1), you can check the distribution\n",
    "print(\"Distribution of Predicted Sentiment:\")\n",
    "print(predicted_labels.value_counts())\n",
    "\n",
    "# \n",
    "# Using internal consistency (percentage of consistent predictions) to measure model\n",
    "internal_consistency = (predicted_labels == 0).mean()  # Adjust as needed\n",
    "print(f\"\\nInternal Consistency: {internal_consistency * 100:.2f}%\")\n",
    "\n",
    "# If 'sentiment' is binary, you can also check the confusion matrix\n",
    "# and classification report\n",
    "conf_matrix = confusion_matrix(predicted_labels, predicted_labels)\n",
    "class_report = classification_report(predicted_labels, predicted_labels)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c204",
   "metadata": {},
   "source": [
    "## Checking for missing values in the date column\n",
    "\n",
    "\n",
    "Time series models work with complete data, and therefore they require the missing data to be replaced with meaningful values before actual analysis. At a high level, missing values in time series are handled in two ways, either dropping them or replacing them. However, dropping missing values can be an inappropriate solution due to the time order of the data and the correlation between observations in adjacent periods.\n",
    "\n",
    " *Imputation* replaces missing values with values estimated from the same data or observed from the environment with the same conditions underlying the missing data.\n",
    "\n",
    "https://www.section.io/engineering-education/missing-values-in-time-series/#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime type\n",
    "pandas_df['date'] = pd.to_datetime(pandas_df['date'])\n",
    "\n",
    "# Create a new column with the date only\n",
    "pandas_df['date_only'] = pandas_df['date'].dt.date\n",
    "\n",
    "# Check the frequency of daily records\n",
    "daily_frequency = pandas_df['date_only'].value_counts().sort_index()\n",
    "\n",
    "# Display the frequency of daily records\n",
    "print(\"Daily Frequency:\\n\", daily_frequency)\n",
    "\n",
    "# Identify missing dates\n",
    "all_dates = pd.date_range(start=pandas_df['date_only'].min(), end=pandas_df['date_only'].max(), freq='D')\n",
    "missing_dates = all_dates[~all_dates.isin(pandas_df['date_only'])]\n",
    "\n",
    "# Display missing dates\n",
    "print(\"\\nMissing Dates:\\n\", missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536daac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the difference between the dates using delta function\n",
    "from datetime import datetime\n",
    "\n",
    "# Corrected date strings\n",
    "d1_str = '2009-04-07'\n",
    "d2_str = '2009-06-25'\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "d1 = datetime.strptime(d1_str, '%Y-%m-%d')\n",
    "d2 = datetime.strptime(d2_str, '%Y-%m-%d')\n",
    "\n",
    "# Difference between dates in timedelta\n",
    "delta = d2 - d1\n",
    "print(f'Difference is {delta.days} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd74e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_frequency.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366045b5",
   "metadata": {},
   "source": [
    "This code represents the date distribution and finding out with dates are missing for the data preparation. When dealing with time series analysis, we must have a complete dataset with all datapoints present in cronological order, otherwise the forecasting will be far less accurate and cause issues for the decision makers. \n",
    "\n",
    "   The date column has a range between <b/>2009-04-07 and 2009-06-25 </b>, which means there are 80 days to use in the timeseries. However, from the Daily Frequency output above, the dataset only containg tweets recorded over 41 days, meaning there are <b> 38 </b> days missing from the total range where there were no records of tweets,. To fix the date column with complete values, data imputation techniques have to be applied. This will be performed in a copy of the current dataframe, to avoid losing any data during the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7b774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(pandas_df.info())\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert 'date_only' column to datetime type\n",
    "pandas_df['date_only'] = pd.to_datetime(pandas_df['date_only'])\n",
    "\n",
    "# Group by date and calculate the count of each sentiment category\n",
    "sentiment_counts = pandas_df.groupby(['date_only', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the evolution of sentiment over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "sentiment_counts.plot(kind='area', stacked=True, colormap='coolwarm', alpha=0.7)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Evolution of Sentiment Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sentiment', loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98861880",
   "metadata": {},
   "source": [
    "The data is grouped by date using value_counts(), and sort_index() is used to ensure the dates are sorted chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0072de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group by date and count the number of occurrences\n",
    "date_distribution = pandas_df['date_only'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "date_distribution.plot(kind='line', marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Date Distribution')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5372b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c6751",
   "metadata": {},
   "source": [
    "## Copy numeric features to a new_df to input missing dates\n",
    "\n",
    "This will create a new DataFrame (new_df) containing only the columns 'date_only', 'sentiment', and 'sentiment_score'. This way there will be only the numeric features needed for next stages. \n",
    "Note that there are still missing dates on the full range of data before doing the timeseries, first step is to add the complete range of 80 days between 2009-04-07 and 2009-06-25.\n",
    "\n",
    "Once the numeric features are extracted from the original dataframe and stored in a copy,the missing dates registered in the lines above can be merged to the new_df. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d483f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of pandas_df extracting most important featueres for thies stage\n",
    "\n",
    "selected_cols = ['date_only','sentiment','sentiment_score']\n",
    "\n",
    "# Create a copy of the DataFrame with only selected columns\n",
    "new_df = pandas_df[selected_cols].copy()\n",
    "\n",
    "\n",
    "#display the new dataframe:\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca7c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(new_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce686f91",
   "metadata": {},
   "source": [
    "After doublechecking all features are in correct datatype, is possible to extract some more from the data.\n",
    "\n",
    "The descriptive statisctics from the sentiment_score column shows a wide range of sentiment scores, with a moderate level of variability. The majority of scores are around 0, suggesting a neutral or mildly positive sentiment, while the 75th percentile indicates a notable proportion of positive sentiment scores.\n",
    "\n",
    "### Storing new_df with sentiment scores back to HDFS\n",
    "\n",
    "As a safety measure for version control, in case the vm starts to run out of space agian, the new dataframe containg only the dates and sentiment scores will be stored on a new csv file back in hdfs. \n",
    "\n",
    "A few techniques to free up space in ubuntu were atempted also, but they damanged some of the packages previously installed (sklearn as the most important for next stages of the project). a new virtual env was created were the missing packages are working aparently, so if necessary the labeled data can be imported to a separate Jupyter Notebook to prevent the progress if the kernel presents any issues. this is due the sentimental analysis codes wih nltk, specially the tokenization and lemmetization codes have a high time of execution of nearly 20 min to run (Total execution time: 1124.5174362659454 seconds as the time function outputs in previous steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'new_df'\n",
    "output_path_downloads = \"/home/hduser/Downloads/SentimentData.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file in the Downloads folder\n",
    "new_df.to_csv(output_path_downloads, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf19f8",
   "metadata": {},
   "source": [
    "Was not able to use the same code as before, system required :~$ pip install fsspec\n",
    "error: externally-managed-environment\n",
    "\n",
    "So the solution was to save the file first to my Downloads folder and from that use a subpreocess to copy the SentimentData back to HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4214339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "#  local file path\n",
    "local_file_path = \"/home/hduser/Downloads/SentimentData.csv\"\n",
    "# Assuming the HDFS directory\n",
    "hdfs_directory = \"hdfs:///user/hduser/CA2\"\n",
    "\n",
    "# Use subprocess to run the hdfs dfs -copyFromLocal command\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-copyFromLocal\", local_file_path, hdfs_directory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['sentiment_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging missing dates to copy of new_df\n",
    "\n",
    "# Step 1: Calculate average sentiment score for each day\n",
    "average_sentiment_per_day = new_df.groupby('date_only')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Step 2: Create a DataFrame with the complete date range with daily freq='D'\n",
    "complete_date_range = pd.date_range(start=new_df['date_only'].min(), end=new_df['date_only'].max(), freq='D')\n",
    "complete_df = pd.DataFrame({'date_only': complete_date_range})\n",
    "\n",
    "# Step 3: Merge the original DataFrame with the complete DataFrame\n",
    "merged_df = pd.merge(complete_df, new_df, on='date_only', how='left')\n",
    "\n",
    "# Step 4: Fill in missing sentiment values with the corresponding daily average\n",
    "merged_df['sentiment'] = merged_df.groupby('date_only')['sentiment'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())\n",
    "print(merged_df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentiments_per_day = new_df.groupby('date_only')[['sentiment', 'sentiment_score']].mean().reset_index()\n",
    "print(average_sentiments_per_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee825442",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by date and count the number of occurrences\n",
    "date_distribution2 = merged_df['date_only'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "date_distribution2.plot(kind='line', marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Date Distribution')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'merged_df' is your DataFrame\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(merged_df['sentiment_score'], bins=20, range=(-1, 1), color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18137471",
   "metadata": {},
   "source": [
    "### Fill in missing sentiment score values with different methods: mean, median, and linear interpolation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'new_df' is your original DataFrame with 'date_only' and 'sentiment_score' columns\n",
    "# Ensure 'date_only' is of datetime type\n",
    "\n",
    "# Step 1: Calculate average sentiment score for each day\n",
    "average_sentiment_per_day = new_df.groupby('date_only')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Step 2: Create a DataFrame with the complete date range with daily freq='D'\n",
    "complete_date_range = pd.date_range(start=new_df['date_only'].min(), end=new_df['date_only'].max(), freq='D')\n",
    "complete_df = pd.DataFrame({'date_only': complete_date_range})\n",
    "\n",
    "# Step 3: Merge the original DataFrame with the complete DataFrame\n",
    "merged_df1 = pd.merge(complete_df, new_df, on='date_only', how='left')\n",
    "\n",
    "\n",
    "# Step 4: Fill in missing sentiment values with different methods: mean, median, and linear interpolation\n",
    "merged_df1['sentiment_mean'] = merged_df1.groupby('date_only')['sentiment_score'].transform('mean').fillna(method='ffill').fillna(method='bfill')\n",
    "merged_df1['sentiment_median'] = merged_df1.groupby('date_only')['sentiment_score'].transform('median').fillna(method='ffill').fillna(method='bfill')\n",
    "merged_df1['sentiment_interpolated'] = merged_df1['sentiment_score'].interpolate(method='linear')\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df1.head())\n",
    "print(merged_df1.info())\n",
    "print(merged_df1.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=merged_df1[['sentiment_mean', 'sentiment_median', 'sentiment_interpolated']])\n",
    "plt.title('Boxplot Comparison of Imputation Methods')\n",
    "plt.xlabel('Imputation Method')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ea99d",
   "metadata": {},
   "source": [
    "For some reason, all the imputations seems to have the same output, it turns out only the interpolation was being used to fill in, the adjusted code in merged_df1 shows all the new imputations compared. The next line used a random datethat was originally missing to see the different results from the imputation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56786c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking a random missing date to see if imputation was succesfull in the last 3 columns:\n",
    "specific_date = '2009-04-30'\n",
    "specific_date_data = merged_df1[merged_df1['date_only'] == specific_date]\n",
    "\n",
    "specific_date_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for original sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df1['sentiment_score'].dropna(), bins=30, alpha=0.5, label='Original', color='blue')\n",
    "\n",
    "# Create a histogram for imputed sentiment scores\n",
    "plt.hist(merged_df1['sentiment_mean'].dropna(), bins=30, alpha=0.5, label='Imputed', color='orange')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Distribution of Sentiment Scores Before and After Imputation')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46514b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate average sentiment score for each day\n",
    "average_sentiment_per_day = new_df.groupby('date_only')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Step 2: Create a DataFrame with the complete date range with daily freq='D'\n",
    "complete_date_range = pd.date_range(start=new_df['date_only'].min(), end=new_df['date_only'].max(), freq='D')\n",
    "complete_df = pd.DataFrame({'date_only': complete_date_range})\n",
    "\n",
    "# Step 3: Merge the original DataFrame with the complete DataFrame\n",
    "merged_df2 = pd.merge(complete_df, new_df, on='date_only', how='left')\n",
    "\n",
    "# Step 4: Fill in missing sentiment values with the corresponding daily average\n",
    "merged_df2['sentiment_score'] = merged_df2['sentiment_score'].fillna(new_df['sentiment_score'].mean())\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df2.head())\n",
    "print(merged_df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9160f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865475b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a line plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='date_only', y='sentiment_score', data=merged_df2, label='Sentiment Score (after Mean Imputation)', marker='o')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Sentiment Score Over Time (with imputation)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average sentiment score for each day for after imputation\n",
    "average_sentiment = merged_df2.groupby('date_only')['sentiment_score'].mean().reset_index()\n",
    "average_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'merged_df' is your DataFrame\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(merged_df2['sentiment_score'], bins=20, range=(-1, 1), color='red', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8c2a2",
   "metadata": {},
   "source": [
    "### Using the first merged_df1 to check how the values are distributed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each sentiment inputation\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# Plot for Sentiment Mean\n",
    "axes[0].plot(merged_df1['date_only'], merged_df1['sentiment_mean'], marker='o', linestyle='-', color='green')\n",
    "axes[0].set_title('Sentiment Mean')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "\n",
    "# Plot for Sentiment Median\n",
    "axes[1].plot(merged_df1['date_only'], merged_df1['sentiment_median'], marker='o', linestyle='-', color='blue')\n",
    "axes[1].set_title('Sentiment Median')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Sentiment Score')\n",
    "\n",
    "# Plot for Sentiment Interpolated\n",
    "axes[2].plot(merged_df1['date_only'], merged_df1['sentiment_interpolated'], marker='o', linestyle='-', color='orange')\n",
    "axes[2].set_title('Sentiment Interpolated')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Sentiment Score')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'new_df'\n",
    "output_path_downloads = \"/home/hduser/Downloads/ImputData.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file in the Downloads folder\n",
    "merged_df2.to_csv(output_path_downloads, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the complete dataframe with the imputations of missing values back in hdfs as well.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Assuming the local file path\n",
    "local_file_path = \"/home/hduser/Downloads/ImputData.csv\"\n",
    "# Assuming the HDFS directory\n",
    "hdfs_directory = \"hdfs:///user/hduser/CA2\"\n",
    "\n",
    "# Use subprocess to run the hdfs dfs -copyFromLocal command\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-copyFromLocal\", local_file_path, hdfs_directory])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139705ba",
   "metadata": {},
   "source": [
    "## Data preparation for Time Series Models\n",
    "\n",
    "When attempeted to free up space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install statsmodels\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd72bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9ac34-d1b6-4fce-8baf-ce9359af210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebd3fd-d8fc-496c-8354-f6bb4cd07dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_pvalue = stats.shapiro(melted_df)\n",
    "print(f'Shapiro-Wilk test statistic: {shapiro_stat}')\n",
    "print(f'Shapiro-Wilk test p-value: {shapiro_pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc45b61-f87e-4184-a76d-a128d4fd634d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077b74-0eba-438f-a452-d5e639521d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ace4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#error: externally-managed-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d24d0032",
   "metadata": {},
   "source": [
    "#apply later stages of analysis, after imputation?\n",
    "result = adfuller(pandas_df)\n",
    "\n",
    "# Extract and print the p-value\n",
    "p_value = result[1]\n",
    "print(\"ADF Test p-value:\", p_value)\n",
    "\n",
    "# Check for stationarity based on the p-value\n",
    "if p_value <= 0.05:\n",
    "    print(\"The time series is likely stationary.\")\n",
    "else:\n",
    "    print(\"The time series is likely non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ba739",
   "metadata": {},
   "source": [
    "- store cleaned dataset back to hbase/mysql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb7845",
   "metadata": {},
   "source": [
    "#### Importing dataset from HBase using a Connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd392e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786231cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7019887d",
   "metadata": {},
   "source": [
    "#### Sentiment extration using SparkNLP library/pipeline\n",
    "\n",
    "The sentiment analysis results will be stored in the 'sentiment.result' column once the data is preprocessed.\n",
    "\n",
    "This pipeline uses the DocumentAssembler to assemble the words into documents, which is required for the Spark NLP SentimentDetector. Then, it applies the sentiment analysis using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ec543-2156-45c8-9344-a00b26145bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "#pipeline = PretrainedPipeline(\"analyze_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c799443-2c64-42d6-b501-f7377412e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import lit\n",
    "\n",
    "# Assuming your existing DataFrame is named `tweets_df` and tokenized words are in the column \"words\"\n",
    "# Add a label column (e.g., 1 for positive, 0 for negative)\n",
    "#labeled_df = tweets_df.withColumn(\"label\", lit(1))  # possibvle to customize this based on sentiment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c8d98-2374-4434-9b96-dffcd93fe868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Combine tokenized words into a single column\n",
    "#labeled_df = labeled_df.withColumn(\"combined_text\", concat_ws(\" \", \"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c211d27-b532-4d70-be32-b770416596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.annotator import SentimentDetector\n",
    "\n",
    "# SentimentDetector setup\n",
    "#sentiment_detector = SentimentDetector() \\\n",
    " #   .setInputCols([\"combined_text\", \"words\"]) \\\n",
    "   # .setOutputCol(\"sentiment\")\n",
    "\n",
    "# Create a new pipeline with the SentimentDetector\n",
    "#pipeline_sentiment = Pipeline(stages=[sentiment_detector])\n",
    "\n",
    "# Apply sentiment detection\n",
    "#analyzed_df = pipeline_sentiment.fit(labeled_df).transform(labeled_df)\n",
    "\n",
    "# Show the results\n",
    "#analyzed_df.select(\"combined_text\", \"sentiment.result\", \"label\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b5b52-5493-4a36-bb5b-a732c0724926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
