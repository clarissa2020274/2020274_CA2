{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0e1860",
   "metadata": {},
   "source": [
    "### MSc Data Analytics \n",
    "\n",
    "##### CA2 - Integrated Assignment sem2\n",
    "\n",
    "#### 2020274 - Clarissa Cardoso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This Notebook contains experimental features for CA2 using different databases to store and retreat files from. The goal of this project is to combine language processing techniques and a time series forecasting to predict the average sentiment of tweets for a certein period of time after the apropriate data cleaning and processing tecniques are applied.\n",
    "\n",
    "Fpr a better understanding the project will be divided into the following sections:\n",
    "\n",
    "- First section consists of importing dataset from various databases, and an attemp to evaluate their performance and usability, helping to select the most suitable dataset for the analysis.\n",
    "\n",
    "- Second section will focus on data cleaning and preprocessing the dataset\n",
    "\n",
    "- Third section focus on deeper EDA features and Natural Language Processing to undertand the dataset better prior to modeling and extract the sentiment from tweets given.\n",
    "\n",
    "- Section four centers on creating the time-series model and selection of apropriate parametrers and hyperparameters to run it.\n",
    "\n",
    "- Fifth section relies on training the model and validating/reacessing features that can be modified for better performance and compare model's results. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7da644",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "\n",
    "The goal of this project is to perform an analysis of the given dataset containing several tweets while experimenting with different databases to store data as well as creating a  time series forecast of the sentiment of the dataset. \n",
    "\n",
    "For the initial experimentation, after instalation of different noSQL databases as seen in class tutorials, I have decided to start with Hbase. One of the reasons why this was the first database used for the project is that it is built on top of HDFS as a part of Hadoop environment and provides a faster lookup on files while displaying lower latency for queries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f9e8f",
   "metadata": {},
   "source": [
    "### Libraries required for project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3ac7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries to deploy pyspark functions\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import count # Funcion to get the \"size\" of the data.\n",
    "from pyspark.sql.functions import when # When function.\n",
    "from pyspark.sql.functions import col # Function column.\n",
    "from pyspark.sql.functions import mean, min, max, stddev # Imports function for statistical features. \n",
    "from pyspark.sql import functions as F # Data processing framework.\n",
    "from pyspark.sql.functions import size, split # Imports function size and split.\n",
    "from pyspark.ml.feature import Tokenizer # Importing Tokenizer.\n",
    "from pyspark.sql.functions import regexp_replace # Remove / Replace function.\n",
    "from pyspark.sql.types import StructField, StructType # Importing features for Schema.\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType # Tools to create the schema.\n",
    "from pyspark.sql.functions import udf # Imports function UDF (user defined functions).\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import max as max_\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "import numpy as np # for numerical operations.\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings # Ignore warnings.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b712d94",
   "metadata": {},
   "source": [
    "#### Importing dataset from HDFS\n",
    "\n",
    "My initial idea, once the dataset given was succesfuly stored in HDFS directory alocated for the CA development (\"CA2/ProjectTweets.csv\"), was to import it straighaway to a noSQL database and perform initial queries inside the HBase enviroment/shell to verify functionality.\n",
    "\n",
    "However my VM had continuous crashes during this process, and the HMaster node managed by Zookeeper kept showing slower times for initializing the commands. After a few seconds the Zookeeper Connection with HDFS and Hase nodes was lost and it was taking me a longer time span to find an alternative. Since the csv file was already in hadoop, I decided to first import from HDFS and the perform some initial cleaning and EDA using Spark framework to process the data to then store the cleneaded data back to HBAse through a connector between Pyspark and the database.\n",
    "\n",
    "\n",
    "- HDFS (Hadoop Distributed File System) is the primary storage system used by Hadoop applications. This open source framework works by rapidly transferring data between nodes. It's often used by companies who need to handle and store big data. <https://www.databricks.com/glossary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc6424",
   "metadata": {},
   "source": [
    "## Import modules, create Spark Session and read file into dataframe\n",
    "\n",
    "First step is to perform some basic exploratory data analysis to get a sense of the data. \n",
    "\n",
    "#### Check the first few rows of the dataset with .show()\n",
    "\n",
    "File was imported with a header marked as 'false' so pyspark will input labels insted of using the first row. This makes room to rename the labels in coming steps. set up schema as true so pyspark utilises the same scema present in the original file, without overlapping the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0863cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession including Legacy for timestamp\n",
    "spark = SparkSession.builder.appName(\"Test Tweets\")\\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"com.johnsnowlabs.nlp.serialization.SparkNLPKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the file path in HDFS\n",
    "file_path = \"hdfs:///user/hduser/CA2/ProjectTweets.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "tweets_test = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame (optional)\n",
    "tweets_test.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e0566",
   "metadata": {},
   "source": [
    "#### Checking the schema of the dataset\n",
    "\n",
    "From this function we see most of the data is composed by strings, which makes sense, since we are working with mostly text. However, on the third column, with the dates of each tweet, we must have a datetime datatype in order to perform the timeseries analysis on further stages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a294330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5889696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|summary|               _c0|                 _c1|                 _c2|     _c3|                 _c4|                 _c5|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|1.9988175522956276E9|                null|    null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|1.9357607362267256E8|                null|    null|5.162733218454889E10|                null|\n",
      "|    min|                 0|          1467810369|Fri Apr 17 20:30:...|NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|           1599999|          2329205794|Wed May 27 07:27:...|NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get summary statistics\n",
    "tweets_test.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a2833",
   "metadata": {},
   "source": [
    "### Part I : Data Pre-Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59afc2",
   "metadata": {},
   "source": [
    "\n",
    "Rename cols and drop c1\n",
    "\n",
    "Convert the date column to a timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508dd527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|index|                date|query_flag|           user|                text|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|    0|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|    5|Mon Apr 06 22:20:...|  NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|Mon Apr 06 22:20:...|  NO_QUERY|        mybirch|         Need a hug |\n",
      "|    7|Mon Apr 06 22:20:...|  NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|Mon Apr 06 22:20:...|  NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|Mon Apr 06 22:20:...|  NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|   10|Mon Apr 06 22:20:...|  NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|   11|Mon Apr 06 22:20:...|  NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|   12|Mon Apr 06 22:20:...|  NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|   13|Mon Apr 06 22:20:...|  NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|Mon Apr 06 22:20:...|  NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|   15|Mon Apr 06 22:20:...|  NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|Mon Apr 06 22:20:...|  NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|   17|Mon Apr 06 22:20:...|  NO_QUERY|       armotley|about to file taxes |\n",
      "|   18|Mon Apr 06 22:20:...|  NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|Mon Apr 06 22:20:...|  NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.drop(\"_c1\") \\\n",
    "           .withColumnRenamed(\"_c0\", \"index\") \\\n",
    "           .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "           .withColumnRenamed(\"_c3\", \"query_flag\") \\\n",
    "           .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "           .withColumnRenamed(\"_c5\", \"text\") \\\n",
    "\n",
    "tweets_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607de02",
   "metadata": {},
   "source": [
    "View a sample of the 'date' column, using the sample() function to double check the timezone used before conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e018e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                date|\n",
      "+--------------------+\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:25:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:31:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see 10% of date row to see correct timezone before converting\n",
    "tweets_test.select(\"date\").sample(False, 0.1, seed=42).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093c5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 06 22:19:45 PDT 2009\n",
      "Mon Apr 06 22:19:49 PDT 2009\n",
      "Mon Apr 06 22:19:53 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:20:00 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:05 PDT 2009\n",
      "Mon Apr 06 22:20:09 PDT 2009\n"
     ]
    }
   ],
   "source": [
    "# limit to 10 rows of date col.\n",
    "sample_date_values = tweets_test.select(\"date\").limit(10).collect()\n",
    "for row in sample_date_values:\n",
    "    print(row.date)\n",
    "\n",
    "    #with this we can confirm the PDT - Pacific Day Time for apropriate conversion to timestamp.\n",
    "    # this may influence further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d861e2",
   "metadata": {},
   "source": [
    "it's important to account for the PDT timezone used. When converting to datetime, the new schema was in the apropriate datatypes, however when i tried to sample the 'date' rows again i got an error as seen below: \n",
    "\n",
    "> <font color='red'> <b>Py4JJavaError:</b> An error occurred while calling o100.showString.\n",
    ": org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM dd HH:mm:ss z yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html </font> \n",
    "\n",
    "So according to Apache Spark documentation, I added a date parsing from java with SimpleDateFormat class to allow customization of the date format of the strings. For that the timezone needs to be specified to avoid any discrepancies. In this case, PDT is UTC-7 which is represented by 'z' in the Apache datetime patterns doc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b953588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.withColumn(\"date\", to_timestamp(tweets_test.date, \"EEE MMM dd HH:mm:ss z yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5df066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715351a",
   "metadata": {},
   "source": [
    "#### Checking for missing values and shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e25325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----+----+\n",
      "|index|date|query_flag|user|text|\n",
      "+-----+----+----------+----+----+\n",
      "|    0|   0|         0|   0|   0|\n",
      "+-----+----+----------+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "# Check for missing values in each column\n",
    "tweets_test.select([count(when(col(c).isNull(), c)).alias(c) for c in tweets_test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e0e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1600000\n",
      "Number of Columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the sahpe of the dataset\n",
    "num_rows = tweets_test.count()\n",
    "num_cols = len(tweets_test.columns)\n",
    "\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feb13d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa663cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|index|               date|query_flag|           user|                text|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|    0|2009-04-07 05:19:45|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|2009-04-07 05:19:49|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|2009-04-07 05:19:53|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|2009-04-07 05:19:57|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|2009-04-07 05:19:57|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test.dropna()  # Drop rows containing NaN values for simplicity\n",
    "tweets_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbcb54",
   "metadata": {},
   "source": [
    "#### Pyspark has some inbuilt functions for starting tne text processing, such as lowercasing, removing special characters and stopwords. \n",
    "\n",
    "The following text data preprocessing steps using PySpark functions are:\n",
    "\n",
    "Lowercasing: We use the lower() function to convert all text to lowercase.\n",
    "\n",
    "Removing Special Characters: We use regexp_replace() to remove any characters that are not alphanumeric or whitespace.\n",
    "\n",
    "Removing Stopwords: We use the StopWordsRemover from the pyspark.ml.feature module to remove common stopwords.\n",
    "\n",
    "The resulting DataFrame tweets_test will have the preprocessed text in the 'text' column.\n",
    "\n",
    "PySpark doesn't have built-in support for stemming or lemmatization. To implement these kind of techniques, external libraries such as nltk can be implemented/imported. \n",
    "<b>NLTK<b/> provides a wide range of tools and resources for working with human language data, and it can complement Spark's capabilities in certain scenarios. \n",
    "\n",
    "However, when i tried to import the nltk functions to my vm, i encountered a series of incompatibility issues. Even after creating a virtual environment i was not able to install the library, and the same happened when i tried to install <b>Sparknlp<b/>, which is the language processing tool whitin the Spark enviroment considered to be the state of the art for a number of functionalities in the NLP area. \n",
    "\n",
    "There was also an attempt, as suggested by SparkNLP documentaroin, to inicialize pyspark with the additional packages for language processing but when i tried to import it back to the notebook it would not find the module installed, even when i apply the same command (pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4) to the virtual env the packages were installed. A third nlp library was also atempted to import via pip: <b>textblob<b/>\n",
    "    \n",
    "#error: externally-managed-environment (#error: externally-managed-environment)\n",
    "\n",
    "With the command found in stackoverflow (sudo apt install python3-nltk) available at https://askubuntu.com/questions/996185/how-can-i-install-nltk-for-python-3\n",
    "\n",
    "i was able to go back to my original choice of applying nltk tools for extracting the sentiment of the tweets given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1995f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4 using virtual enviroment 'myenv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7bdaa",
   "metadata": {},
   "source": [
    "Tokenization is performed using the Tokenizer class.\n",
    "\n",
    "HTML parsing is done using the BeautifulSoup library, and a user-defined function (parse_html_udf) is registered and applied to create a new column named \"cleaned_text.\"\n",
    "\n",
    "A user-defined function is applied to remove special characters and numbers from the \"cleaned_text\" column.\n",
    "Stop words are removed using the StopWordsRemover class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "913152d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert text to lowercase\n",
    "#df_cleaned = df_cleaned.withColumn('cleaned_words', lower('cleaned_words'))\n",
    "\n",
    "# Remove duplicate rows based on the 'cleaned_words' column\n",
    "#df_cleaned = df_cleaned.dropDuplicates(['cleaned_words'])\n",
    "\n",
    "# Remove rows with empty 'cleaned_words'\n",
    "#df_cleaned = df_cleaned.filter(df_cleaned.cleaned_words != '')\n",
    "\n",
    "# Show the result\n",
    "#df_cleaned.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690a9b3",
   "metadata": {},
   "source": [
    "Removing noise from data: \n",
    "    -Stop words\n",
    "    -Special characters\n",
    "    -transform all to lower case letters\n",
    "    -remove numbers, duplicate characters and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb543ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|my whole body feels itchy and like its on fire                                                             |\n",
      "| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "| not the whole crew                                                                                        |\n",
      "|need a hug                                                                                                 |\n",
      "| hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you                           |\n",
      "| nope they didnt have it                                                                                   |\n",
      "| que me muera                                                                                              |\n",
      "|spring break in plain city11 its snowing                                                                   |\n",
      "|i just repierced my ears                                                                                   |\n",
      "| i couldnt bear to watch it  and i thought the ua loss was embarrassing                                    |\n",
      "| it it counts idk why i did either you never talk to me anymore                                            |\n",
      "| i wouldve been the first but i didnt have a gun11not really though zac snyders just a doucheclown         |\n",
      "| i wish i got to watch it with you i miss you and11how was the premiere                                    |\n",
      "|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                |\n",
      "|about to file taxes                                                                                        |\n",
      "| ahh ive always wanted to see rent  love the soundtrack                                                    |\n",
      "| oh dear were you drinking out of the forgotten table drinks                                               |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"http(s)?://[^\\\\s]+\", \"\"))\n",
    "\n",
    "# Remove HTML tags\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# Remove mentions (i.e., @username)\n",
    "tweets_test = tweets_test.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"@\\\\w+\", \"\"))\n",
    "\n",
    "# Convert to lowercase\n",
    "tweets_test = tweets_test.withColumn('text', lower(tweets_test['text']))\n",
    "\n",
    "# Remove numbers from the \"text\" column\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace(tweets_test['text'], r'\\d+', ''))\n",
    "\n",
    "# Reduce excessive characters (more than two of the same in a row)\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace('text', r'(.)\\1{2,}', r'\\1\\1'))\n",
    "\n",
    "# Remove punctuation\n",
    "tweets_test = tweets_test.withColumn('text', regexp_replace(tweets_test['text'], r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "tweets_test.select(\"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef984308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240940f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'flag' and 'user' columns\n",
    "tweets_test = tweets_test.drop('query_flag', 'user')\n",
    "\n",
    "# Show the result\n",
    "tweets_test.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6d1ae",
   "metadata": {},
   "source": [
    "### Write Cleaned dataset in a new csv back to hdfs. \n",
    "\n",
    "with code adapted from SparkByExamples[], let's store the cleaned tweets to a new csv file back to hdfs. \n",
    "This will be later on imported to mysql for testing different databases. the code will be left eith comments just to protect the vm, and avoid overwriting the files in case the kernel is restarted. \n",
    "\n",
    "[] https://sparkbyexamples.com/spark/spark-write-dataframe-to-csv-file/#:~:text=In%20Spark%2C%20you%20can%20save,any%20Spark%20supported%20file%20systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "646c90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path in HDFS where you want to save the new DataFrame\n",
    "#new_file_path = \"hdfs:///user/hduser/CA2/CleanTweets.csv\"\n",
    "\n",
    "# Save the DataFrame to HDFS as a CSV file\n",
    "#tweets_test.write.csv(new_file_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8c44ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The path in HDFS where you want to store the clean DataFrame\n",
    "output_directory = \"hdfs:///user/hduser/CA2\"\n",
    "\n",
    "# Save the DataFrames to HDFS as CSV files\n",
    "tweets_test.write.csv(f\"{output_directory}/CleanTweets\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f352c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "|5    |2009-04-07 05:20:00| not the whole crew                                                                                        |\n",
      "|6    |2009-04-07 05:20:03|need a hug                                                                                                 |\n",
      "|7    |2009-04-07 05:20:03| hey  long time no see yes rains a bit only a bit  lol  im fine thanks  hows you                           |\n",
      "|8    |2009-04-07 05:20:05| nope they didnt have it                                                                                   |\n",
      "|9    |2009-04-07 05:20:09| que me muera                                                                                              |\n",
      "|10   |2009-04-07 05:20:16|spring break in plain city11 its snowing                                                                   |\n",
      "|11   |2009-04-07 05:20:17|i just repierced my ears                                                                                   |\n",
      "|12   |2009-04-07 05:20:19| i couldnt bear to watch it  and i thought the ua loss was embarrassing                                    |\n",
      "|13   |2009-04-07 05:20:19| it it counts idk why i did either you never talk to me anymore                                            |\n",
      "|14   |2009-04-07 05:20:20| i wouldve been the first but i didnt have a gun11not really though zac snyders just a doucheclown         |\n",
      "|15   |2009-04-07 05:20:20| i wish i got to watch it with you i miss you and11how was the premiere                                    |\n",
      "|16   |2009-04-07 05:20:22|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                |\n",
      "|17   |2009-04-07 05:20:25|about to file taxes                                                                                        |\n",
      "|18   |2009-04-07 05:20:31| ahh ive always wanted to see rent  love the soundtrack                                                    |\n",
      "|19   |2009-04-07 05:20:34| oh dear were you drinking out of the forgotten table drinks                                               |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "tweets_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773d166",
   "metadata": {},
   "source": [
    "## Convert to pandas df for apply nltk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f65a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = tweets_test.toPandas()\n",
    "\n",
    "# Now 'pandas_df' is a Pandas DataFrame\n",
    "# You can apply NLTK or other Python libraries on 'pandas_df' as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69701efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-07 05:19:45</td>\n",
       "      <td>a11 thats a bummer  you shoulda got david c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-04-07 05:19:49</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2009-04-07 05:19:53</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                date  \\\n",
       "0      0 2009-04-07 05:19:45   \n",
       "1      1 2009-04-07 05:19:49   \n",
       "2      2 2009-04-07 05:19:53   \n",
       "3      3 2009-04-07 05:19:57   \n",
       "4      4 2009-04-07 05:19:57   \n",
       "\n",
       "                                                text  \n",
       "0     a11 thats a bummer  you shoulda got david c...  \n",
       "1  is upset that he cant update his facebook by t...  \n",
       "2   i dived many times for the ball managed to sa...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4   no its not behaving at all im mad why am i he...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count    Dtype         \n",
      "---  ------  --------------    -----         \n",
      " 0   index   1600000 non-null  int32         \n",
      " 1   date    1600000 non-null  datetime64[ns]\n",
      " 2   text    1600000 non-null  object        \n",
      "dtypes: datetime64[ns](1), int32(1), object(1)\n",
      "memory usage: 30.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "display(pandas_df.head())\n",
    "print(pandas_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b95dda",
   "metadata": {},
   "source": [
    "### Text Preprocessing for sentiment extraction using NLTK library\n",
    "\"Text preprocessing is a crucial step in performing sentiment analysis, as it helps to clean and normalize the text data, making it easier to analyze. The preprocessing step involves a series of techniques that help transform raw text data into a form you can use for analysis. Some common text preprocessing techniques include tokenization, stop word removal, stemming, and lemmatization.\"https://www.datacamp.com/tutorial/text-analytics-beginners-nltk\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is a text preprocessing step in sentiment analysis that involves breaking down the text into individual words or tokens. This is an essential step in analyzing text data as it helps to separate individual words from the raw text, making it easier to analyze and understand. Tokenization is typically performed using NLTK's built-in `word_tokenize` function, which can split the text into individual words and punctuation marks.\n",
    "\n",
    "### Stop words\n",
    "Stop word removal is a crucial text preprocessing step in sentiment analysis that involves removing common and irrelevant words that are unlikely to convey much sentiment. Stop words are words that are very common in a language and do not carry much meaning, such as \"and,\" \"the,\" \"of,\" and \"it.\" These words can cause noise and skew the analysis if they are not removed.\n",
    "\n",
    "By removing stop words, the remaining words in the text are more likely to indicate the sentiment being expressed. This can help to improve the accuracy of the sentiment analysis. NLTK provides a built-in list of stop words for several languages, which can be used to filter out these words from the text data.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "Stemming and lemmatization are techniques used to reduce words to their root forms. Stemming involves removing the suffixes from words, such as \"ing\" or \"ed,\" to reduce them to their base form. For example, the word \"jumping\" would be stemmed to \"jump.\" \n",
    "\n",
    "Lemmatization, however, involves reducing words to their base form based on their part of speech. For example, the word \"jumped\" would be lemmatized to \"jump,\" but the word \"jumping\" would be lemmatized to \"jumping\" since it is a present participle.\n",
    "\n",
    "To learn more about stemming and lemmatization, check out our Stemming and Lemmatization in Python tutorial.\n",
    "\n",
    "\n",
    "#### Bag of Words (BoW) Model\n",
    "The bag of words model is a technique used in natural language processing (NLP) to represent text data as a set of numerical features. In this model, each document or piece of text is represented as a \"bag\" of words, with each word in the text represented by a separate feature or dimension in the resulting vector. The value of each feature is determined by the number of times the corresponding word appears in the text.\n",
    "\n",
    "The bag of words model is useful in NLP because it allows us to analyze text data using machine learning algorithms, which typically require numerical input. By representing text data as numerical features, we can train machine learning models to classify text or analyze sentiments. \n",
    "\n",
    "The example in the next section will use the NLTK Vader model for sentiment analysis on the Amazon customer dataset. In this particular example, we do not need to perform this step because the NLTK Vader API accepts text as an input instead of numeric vectors, but if you were building a supervised machine learning model to predict sentiment (assuming you have labeled data), you would have to transform the processed text into a bag of words model before training the machine learning model. ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a9f384f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763fb56",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Let’s create a function preprocess_text in which we first tokenize the documents using word_tokenize function from NLTK, then we remove step words using stepwords module from NLTK and finally, we lemmatize the filtered_tokens using WordNetLemmatizer from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fdef06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-07 05:19:45</td>\n",
       "      <td>a11 thats bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-04-07 05:19:49</td>\n",
       "      <td>upset cant update facebook texting it11 might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2009-04-07 05:19:53</td>\n",
       "      <td>dived many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1599995</td>\n",
       "      <td>2009-06-16 15:40:49</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1599996</td>\n",
       "      <td>2009-06-16 15:40:49</td>\n",
       "      <td>thewdbcom cool hear old walt interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1599997</td>\n",
       "      <td>2009-06-16 15:40:49</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1599998</td>\n",
       "      <td>2009-06-16 15:40:49</td>\n",
       "      <td>happy th birthday boo a11 time11 tupac amaru s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1599999</td>\n",
       "      <td>2009-06-16 15:40:50</td>\n",
       "      <td>happy charitytuesday11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index                date  \\\n",
       "0              0 2009-04-07 05:19:45   \n",
       "1              1 2009-04-07 05:19:49   \n",
       "2              2 2009-04-07 05:19:53   \n",
       "3              3 2009-04-07 05:19:57   \n",
       "4              4 2009-04-07 05:19:57   \n",
       "...          ...                 ...   \n",
       "1599995  1599995 2009-06-16 15:40:49   \n",
       "1599996  1599996 2009-06-16 15:40:49   \n",
       "1599997  1599997 2009-06-16 15:40:49   \n",
       "1599998  1599998 2009-06-16 15:40:49   \n",
       "1599999  1599999 2009-06-16 15:40:50   \n",
       "\n",
       "                                                      text  \n",
       "0        a11 thats bummer shoulda got david carr third day  \n",
       "1        upset cant update facebook texting it11 might ...  \n",
       "2          dived many time ball managed save rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                 behaving im mad cant see  \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996             thewdbcom cool hear old walt interview  \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998  happy th birthday boo a11 time11 tupac amaru s...  \n",
       "1599999                             happy charitytuesday11  \n",
       "\n",
       "[1600000 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download nltk corpus (first time only)\n",
    "import nltk\n",
    "\n",
    "\n",
    "# create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Remove stop words\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# apply the function to \n",
    "\n",
    "pandas_df['text'] = pandas_df['text'].apply(preprocess_text)\n",
    "pandas_df.head()\n",
    "\n",
    "#code adapted from Datacamp NLTK Sentiment Analysis Tutorial for Beginners- \n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035265b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The NLTK sentiment analyzer below returns a score between -1 and +1. We have used a cut-off threshold of 0 in the get_sentiment function. Anything above 0 is classified as 1 (meaning positive). Since we have actual labels, we can evaluate the performance of this method by building a confusion matrix. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ece5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2009-04-07 05:19:45</td>\n",
       "      <td>a11 thats bummer shoulda got david carr third day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-04-07 05:19:49</td>\n",
       "      <td>upset cant update facebook texting it11 might ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2009-04-07 05:19:53</td>\n",
       "      <td>dived many time ball managed save rest go bound</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2009-04-07 05:19:57</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                date  \\\n",
       "0      0 2009-04-07 05:19:45   \n",
       "1      1 2009-04-07 05:19:49   \n",
       "2      2 2009-04-07 05:19:53   \n",
       "3      3 2009-04-07 05:19:57   \n",
       "4      4 2009-04-07 05:19:57   \n",
       "\n",
       "                                                text  sentiment  \n",
       "0  a11 thats bummer shoulda got david carr third day          0  \n",
       "1  upset cant update facebook texting it11 might ...          0  \n",
       "2    dived many time ball managed save rest go bound          1  \n",
       "3                    whole body feel itchy like fire          1  \n",
       "4                           behaving im mad cant see          0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# create get_sentiment function\n",
    "def get_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0\n",
    "    return sentiment\n",
    "\n",
    "# apply get_sentiment function\n",
    "\n",
    "pandas_df['sentiment'] = pandas_df['text'].apply(get_sentiment)\n",
    "\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f961777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    881786\n",
       "0    718214\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dist of sentiment:\n",
    "\n",
    "pandas_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddace0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGwCAYAAAC6ty9tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPC0lEQVR4nO3df1yV9f3/8ccR4YgkJxQBj1naj5mElUFDtIWlgCWSn23pRjIp42NhGQPTsVY5V6DmsA3KylU2teH2cfTphxJESyM9SnygiemqpSEDRAtBmR0Iz/cPv17tiL+wcw52et53u263uK4X7+t9TrNevV7v93WZHA6HAxEREREv0KunJyAiIiLiKkpsRERExGsosRERERGvocRGREREvIYSGxEREfEaSmxERETEayixEREREa+hxEZERES8Ru+enoA7DLxzbU9PQeS8tHfFtJ6egsh5p48H/k3oP+o+l4xzpKrAJeN4M1VsRERExGt4ZcVGRETkvGJSHcFTlNiIiIi4m8nU0zP4zlBiIyIi4m6q2HiMvmkRERHxGqrYiIiIuJtaUR6jxEZERMTd1IryGH3TIiIi4jVUsREREXE3taI8RomNiIiIu6kV5TH6pkVERMRrqGIjIiLibmpFeYwSGxEREXdTK8pj9E2LiIiI11DFRkRExN3UivIYJTYiIiLuplaUxyixERERcTdVbDxGKaSIiIh4DVVsRERE3E2tKI9RYiMiIuJuSmw8Rt+0iIiIeA1VbERERNytlxYPe4oSGxEREXdTK8pj9E2LiIiI11DFRkRExN30HBuPUWIjIiLibmpFeYy+aREREfEaqtiIiIi4m1pRHqPERkRExN3UivIYJTYiIiLupoqNxyiFFBER8UJfffUVv/rVrxg2bBj+/v5ceumlLFy4kKNHjxoxDoeDBQsWYLVa8ff3Z9y4cezYscNpHLvdzv33309wcDABAQEkJSVRV1fnFNPc3ExKSgoWiwWLxUJKSgoHDx50iqmtrWXy5MkEBAQQHBzMnDlzaG9vd4rZvn07sbGx+Pv7M3jwYBYuXIjD4ejW51ZiIyIi4m6mXq45umHx4sU888wzFBQUsHPnTpYsWcITTzxBfn6+EbNkyRLy8vIoKCigoqKCsLAw4uLiOHTokBGTkZFBUVERhYWFlJeXc/jwYRITE+ns7DRikpOTqa6upri4mOLiYqqrq0lJSTGud3Z2MmnSJNra2igvL6ewsJB169aRlZVlxLS2thIXF4fVaqWiooL8/HyWLl1KXl5e975qR3dToW+BgXeu7ekpiJyX9q6Y1tNTEDnv9PHAogz/W5a5ZJyDr6Rjt9udzpnNZsxmc5fYxMREQkNDef75541zP/rRj+jbty+rVq3C4XBgtVrJyMhg/vz5wLHqTGhoKIsXL2bWrFm0tLQwcOBAVq1axbRpx/75UV9fz5AhQ1i/fj0JCQns3LmT8PBwbDYb0dHRANhsNmJiYti1axfDhw9nw4YNJCYmsnfvXqxWKwCFhYWkpqbS1NREYGAgy5cvJzs7m3379hmfZ9GiReTn51NXV4fpLNt5qtiIiIh8S+Tm5hrtnuNHbm7uSWNvuOEGysrK+OijjwD44IMPKC8v59ZbbwVg9+7dNDY2Eh8fb/yO2WwmNjaWzZs3A1BZWUlHR4dTjNVqJSIiwojZsmULFovFSGoARo8ejcVicYqJiIgwkhqAhIQE7HY7lZWVRkxsbKxTkpaQkEB9fT179uw56+9Ii4dFRETczUW7orKzs8nMzHQ6d7JqDcD8+fNpaWnhyiuvxMfHh87OTh5//HF++tOfAtDY2AhAaGio0++Fhoby2WefGTF+fn4EBQV1iTn++42NjYSEhHS5f0hIiFPMifcJCgrCz8/PKWbo0KFd7nP82rBhw076OU+kxEZERMTdXLQr6lRtp5NZu3Ytq1ev5uWXX+aqq66iurqajIwMrFYrM2bM+I+pOc/N4XCcse1zYszJ4l0Rc3y1zNm2oUCtKBEREa/04IMP8otf/IKf/OQnjBw5kpSUFH7+858brauwsDDg68rNcU1NTUalJCwsjPb2dpqbm08bs2/fvi73379/v1PMifdpbm6mo6PjtDFNTU1A16rS6SixERERcbce2BX173//m169nH/Hx8fH2O49bNgwwsLCKC0tNa63t7ezceNGxowZA0BkZCS+vr5OMQ0NDdTU1BgxMTExtLS0sG3bNiNm69attLS0OMXU1NTQ0NBgxJSUlGA2m4mMjDRiNm3a5LQFvKSkBKvV2qVFdTpKbERERNytBxKbyZMn8/jjj/PGG2+wZ88eioqKyMvL47/+67+OTclkIiMjg5ycHIqKiqipqSE1NZW+ffuSnJwMgMViYebMmWRlZVFWVkZVVRXTp09n5MiRTJgwAYARI0YwceJE0tLSsNls2Gw20tLSSExMZPjw4QDEx8cTHh5OSkoKVVVVlJWVMXfuXNLS0ggMDASObRk3m82kpqZSU1NDUVEROTk5ZGZmdqsVpTU2IiIiXig/P5+HH36Y9PR0mpqasFqtzJo1i0ceecSImTdvHkeOHCE9PZ3m5maio6MpKSmhX79+RsyyZcvo3bs3U6dO5ciRI4wfP56VK1fi4+NjxKxZs4Y5c+YYu6eSkpIoKCgwrvv4+PDGG2+Qnp7O2LFj8ff3Jzk5maVLlxoxFouF0tJSZs+eTVRUFEFBQWRmZnZZLH0meo6NyHeInmMj0pVHnmOTtNwl4xx59V6XjOPNVLERERFxN70E02OU2IiIiLibXoLpMUohRURExGuoYiMiIuJuakV5jBIbERERd1MrymOUQoqIiIjXUMVGRETEzbrzgDn5ZpTYiIiIuJkSG89RK0pERES8hio2IiIi7qaCjccosREREXEztaI8R60oERER8Rqq2IiIiLiZKjaeo8RGRETEzZTYeI4SGxERETdTYuM5WmMjIiIiXkMVGxEREXdTwcZjlNiIiIi4mVpRnqNWlIiIiHgNVWxERETcTBUbz1FiIyIi4mZKbDxHrSgRERHxGqrYiIiIuJkqNp6jxEZERMTdlNd4jFpRIiIi4jVUsREREXEztaI8R4mNiIiImymx8RwlNiIiIm6mxMZztMZGREREvIYqNiIiIu6mgo3HKLERERFxM7WiPEetKBERES80dOhQTCZTl2P27NkAOBwOFixYgNVqxd/fn3HjxrFjxw6nMex2O/fffz/BwcEEBASQlJREXV2dU0xzczMpKSlYLBYsFgspKSkcPHjQKaa2tpbJkycTEBBAcHAwc+bMob293Slm+/btxMbG4u/vz+DBg1m4cCEOh6Pbn1uJjYiIiJudLME4l6M7KioqaGhoMI7S0lIAbr/9dgCWLFlCXl4eBQUFVFRUEBYWRlxcHIcOHTLGyMjIoKioiMLCQsrLyzl8+DCJiYl0dnYaMcnJyVRXV1NcXExxcTHV1dWkpKQY1zs7O5k0aRJtbW2Ul5dTWFjIunXryMrKMmJaW1uJi4vDarVSUVFBfn4+S5cuJS8vr/vfteNc0qHz3MA71/b0FETOS3tXTOvpKYicd/p4YFHGoP9e55Jx9uQnYrfbnc6ZzWbMZvMZfzcjI4PXX3+djz/+GACr1UpGRgbz588HjlVnQkNDWbx4MbNmzaKlpYWBAweyatUqpk079s+O+vp6hgwZwvr160lISGDnzp2Eh4djs9mIjo4GwGazERMTw65duxg+fDgbNmwgMTGRvXv3YrVaASgsLCQ1NZWmpiYCAwNZvnw52dnZ7Nu3z/gsixYtIj8/n7q6um4ldarYiIiIfEvk5uYaLZ/jR25u7hl/r729ndWrV3PXXXdhMpnYvXs3jY2NxMfHGzFms5nY2Fg2b94MQGVlJR0dHU4xVquViIgII2bLli1YLBYjqQEYPXo0FovFKSYiIsJIagASEhKw2+1UVlYaMbGxsU4JWkJCAvX19ezZs6db35EWD4uIiLiZqxYPZ2dnk5mZ6XTubKo1r7zyCgcPHiQ1NRWAxsZGAEJDQ53iQkND+eyzz4wYPz8/goKCusQc//3GxkZCQkK63C8kJMQp5sT7BAUF4efn5xQzdOjQLvc5fm3YsGFn/IzHKbERERFxNxdtijrbttOJnn/+eW655Ranqgl0TbgcDscZk7ATY04W74qY4ytlupsUqhUlIiLixT777DPeeust7r77buNcWFgY8HXl5rimpiajUhIWFkZ7ezvNzc2njdm3b1+Xe+7fv98p5sT7NDc309HRcdqYpqYmoGtV6UyU2IiIiLhZT+yKOu7FF18kJCSESZMmGeeGDRtGWFiYsVMKjq3D2bhxI2PGjAEgMjISX19fp5iGhgZqamqMmJiYGFpaWti2bZsRs3XrVlpaWpxiampqaGhoMGJKSkowm81ERkYaMZs2bXLaAl5SUoLVau3SojoTJTYiIiJu1lOJzdGjR3nxxReZMWMGvXt/vfrEZDKRkZFBTk4ORUVF1NTUkJqaSt++fUlOTgbAYrEwc+ZMsrKyKCsro6qqiunTpzNy5EgmTJgAwIgRI5g4cSJpaWnYbDZsNhtpaWkkJiYyfPhwAOLj4wkPDyclJYWqqirKysqYO3cuaWlpBAYGAse2jJvNZlJTU6mpqaGoqIicnBwyMzO7/bm1xkZERMTNeurJw2+99Ra1tbXcddddXa7NmzePI0eOkJ6eTnNzM9HR0ZSUlNCvXz8jZtmyZfTu3ZupU6dy5MgRxo8fz8qVK/Hx8TFi1qxZw5w5c4zdU0lJSRQUFBjXfXx8eOONN0hPT2fs2LH4+/uTnJzM0qVLjRiLxUJpaSmzZ88mKiqKoKAgMjMzuyyUPht6jo3Id4ieYyPSlSeeYzNk9v+6ZJy9T93mknG8mSo2IiIi7qZXRXmMEhsRERE300swPUeLh0VERMRrqGLzHVb5RCIXBwd0Of9C2cfMX/1/TIoczIxxl3H1Jf0Z0M/MTY+8Sc3eg06xIYF9eHTaNYy7KpSAPr78s/EQT77+Ia+9f+ztr0MG9CUr6SpuGBFCiKUPjQe/5H+27GHZazvp6DxqjPN48iiirwjmysEWPm5o5aZHS04572EhF/D2gng6HQ4un13kmi9DxEPW/mkNK198ngP793PZ5Vcw7xe/5LrIqJ6elriZKjaeo8TmOyx+YSk+//GH7cqLLKx7cBz/W7EXgL5+vdn68QFerahj2Z3Xn3SMp/47mkB/X6b/rpwvDtv50ehLWHFvDHG/LmV77UGuGBRIL5OJuS+9z+6mw1w52EJe6vX0NfdmwdoPjHFMwMvv7ua6Swdw1RDLKefc28fEs/fEYPt4P9dfHuyaL0LEQ4o3rGfJolweevhRrh11Hf/z50LSZ6VR9OobDDrhibDiXZTYeI5aUd9hnx+y09T6pXHEX2Nl975DbP7HfgD+suUzfvvqh2zc0XjKMa6/bAB/eOtjqnZ/wWf728h77UNa/t3B1Zcce7fI2zWNzHlhG+/s2Mdn+9t4s7qep4t3Mem6i5zG+eXLVbzw9id8tv/waeec/cORfNzQyv9u2/sNP72I56166UX+60c/4oc/vp1LL7uMedkPETYojD+v/VNPT03EayixEQB8fXrx45hLePnd3d36va0fH2DK9y/mwgA/TCaY8v0hmHv34r1dTaf8ncC+vhxsaz/l9VO5YUQISVFDmL+qstu/K9LTOtrb2fnhDmLG3OB0PmbMWD6oruqhWYmn9OSTh79rerQVVVdXx/Lly9m8eTONjY2YTCZCQ0MZM2YM99xzD0OGDOnJ6X2n3HrdYCx9ffnTe91LbO5evoU/3BvDxwX/RcdXRznS/hUz8t9jz/62k8YPHRjA3eOv4NH/aEOdjaAAP/Jnfp/057Zy+MuvuvW7IueD5oPNdHZ2MmDAAKfzAwYEc+DA/h6alXiMchKP6bHEpry8nFtuuYUhQ4YQHx9PfHw8DoeDpqYmXnnlFfLz89mwYQNjx4497Th2ux273e50ztHZgcnH153T9zp33DiMsu0N7Dv4Zbd+75c/HImlrx8/XPI3vjjczi3XDeb52WOYnPs2O+tanGJDL+zD2qxYXn2/jtWbPu3WffLuvJ6/2mrZ8pH+BSDfbufyNmUROXs9ltj8/Oc/5+6772bZsmWnvJ6RkUFFRcVpx8nNzeXXv/610zn/a35EwKjbXTZXb3fRgL7cGB5KasF73fq9oQMDuHvCFdzw0Ab+Ud8KwI69Bxl9RTB33Xw5D/7x65ZR6IV9eGXeTVR88jmZK0//9/RkfjAihInXWkmfeOzdIyYT+PTqRcMfbifrpfe73UIT8bSgC4Pw8fHhwIEDTue/+OJzBgzQQnhvp+TVc3ossampqWH16tWnvD5r1iyeeeaZM46TnZ3d5V0Sl9736jee33fJT28YxoFWO6UfNJw5+D/4m4/93+foCW/lOOpw0Os//hCHXejPK/Nv4oM9XzDn+W2cy0s8bnnsLXx6fb0k7JZRVu6/dQS3Pl5GQ/O/uz+giIf5+vkxIvwqbJvfY/yEOOO8bfNmxt08vgdnJp6gxMZzeiyxGTRoEJs3bzbe/nmiLVu2MGjQoDOOYzabMZvNTufUhjp7JtOxxGbte3voPOqccVwY4MdF/fsSFuQPwOWDjr0Yranl2C6qjxta+XTfIX47I4pH135A82E7t1x3EbHhYdzxu3eBY5Wa//3FTdR9/m8eXfsBwf2+/nvV1Pp122tYyAUEmHsTYulDH18fIoZcCMA/6lvp6DzKxw2HnOZ27dAgjjoc7PqXc7tL5HyWMuNOHvrFPMIjIrjmmlGs+8taGhoauH3aT3p6auJmyms8p8cSm7lz53LPPfdQWVlJXFwcoaGhmEwmGhsbKS0t5Q9/+ANPPvlkT03vOyM2PJQhwQGsebfrmpeJ11rJvzva+HnFvWMAWPJKDU/87w6+6nTw02WbePjHV7P6gR8Q0Kc3u/cd5r4/bOWtvx+r/tx0VRiXhvbj0tB+bF+W5DT+f76sdNmd1zP2yhDj578tTADgurmvsfdzVWTEO0y85VZaDjbz3PKn2b+/icuv+B5PPfMcVuvgnp6aiNfo0bd7r127lmXLllFZWUlnZydw7PXmkZGRZGZmMnXq1HMaV2/3Fjk5vd1bpCtPvN37igeLXTLOx09MdMk43qxHt3tPmzaNadOm0dHRYSyoCw4OxtdXrSQREfEeakV5znnxSgVfX9+zWk8jIiIicjrnRWIjIiLizbQrynOU2IiIiLiZ8hrP0buiRERExGuoYiMiIuJmvXqpZOMpSmxERETcTK0oz1ErSkRERLyGKjYiIiJupl1RnqPERkRExM2U13iOEhsRERE3U8XGc7TGRkRERLyGKjYiIiJupoqN5yixERERcTPlNZ6jVpSIiIh4DVVsRERE3EytKM9RYiMiIuJmyms8R60oERERL/Wvf/2L6dOnM2DAAPr27cu1115LZWWlcd3hcLBgwQKsViv+/v6MGzeOHTt2OI1ht9u5//77CQ4OJiAggKSkJOrq6pximpubSUlJwWKxYLFYSElJ4eDBg04xtbW1TJ48mYCAAIKDg5kzZw7t7e1OMdu3byc2NhZ/f38GDx7MwoULcTgc3frMSmxERETczGQyueTojubmZsaOHYuvry8bNmzgww8/5Le//S0XXnihEbNkyRLy8vIoKCigoqKCsLAw4uLiOHTokBGTkZFBUVERhYWFlJeXc/jwYRITE+ns7DRikpOTqa6upri4mOLiYqqrq0lJSTGud3Z2MmnSJNra2igvL6ewsJB169aRlZVlxLS2thIXF4fVaqWiooL8/HyWLl1KXl5e975rR3dToW+BgXeu7ekpiJyX9q6Y1tNTEDnv9PHAooyox/7mknHee3AMdrvd6ZzZbMZsNneJ/cUvfsF7773Hu+++e9KxHA4HVquVjIwM5s+fDxyrzoSGhrJ48WJmzZpFS0sLAwcOZNWqVUybduyfH/X19QwZMoT169eTkJDAzp07CQ8Px2azER0dDYDNZiMmJoZdu3YxfPhwNmzYQGJiInv37sVqtQJQWFhIamoqTU1NBAYGsnz5crKzs9m3b5/xeRYtWkR+fj51dXVnndipYiMiIvItkZuba7R7jh+5ubknjX311VeJiori9ttvJyQkhFGjRrFixQrj+u7du2lsbCQ+Pt44ZzabiY2NZfPmzQBUVlbS0dHhFGO1WomIiDBitmzZgsViMZIagNGjR2OxWJxiIiIijKQGICEhAbvdbrTGtmzZQmxsrFOSlpCQQH19PXv27Dnr70iJjYiIiJu5qhWVnZ1NS0uL05GdnX3Se3766acsX76cK664gjfffJN77rmHOXPm8Mc//hGAxsZGAEJDQ51+LzQ01LjW2NiIn58fQUFBp40JCQnpcv+QkBCnmBPvExQUhJ+f32ljjv98POZsaFeUiIiIm7lqV9Sp2k4nc/ToUaKiosjJyQFg1KhR7Nixg+XLl/Ozn/3sP+bmPDmHw3HGts+JMSeLd0XM8dUy3VlfpIqNiIiIm/XE4uFBgwYRHh7udG7EiBHU1tYCEBYWBnSthjQ1NRmVkrCwMNrb22lubj5tzL59+7rcf//+/U4xJ96nubmZjo6O08Y0NTUBXatKp6PERkRExAuNHTuWf/zjH07nPvroIy655BIAhg0bRlhYGKWlpcb19vZ2Nm7cyJgxYwCIjIzE19fXKaahoYGamhojJiYmhpaWFrZt22bEbN26lZaWFqeYmpoaGhoajJiSkhLMZjORkZFGzKZNm5y2gJeUlGC1Whk6dOhZf24lNiIiIm5mMrnm6I6f//zn2Gw2cnJy+OSTT3j55Zd57rnnmD179v+fk4mMjAxycnIoKiqipqaG1NRU+vbtS3JyMgAWi4WZM2eSlZVFWVkZVVVVTJ8+nZEjRzJhwgTgWBVo4sSJpKWlYbPZsNlspKWlkZiYyPDhwwGIj48nPDyclJQUqqqqKCsrY+7cuaSlpREYGAgc2zJuNptJTU2lpqaGoqIicnJyyMzM7Fa1SmtsRERE3KwnXqlw/fXXU1RURHZ2NgsXLmTYsGE8+eST3HHHHUbMvHnzOHLkCOnp6TQ3NxMdHU1JSQn9+vUzYpYtW0bv3r2ZOnUqR44cYfz48axcuRIfHx8jZs2aNcyZM8fYPZWUlERBQYFx3cfHhzfeeIP09HTGjh2Lv78/ycnJLF261IixWCyUlpYye/ZsoqKiCAoKIjMzk8zMzG59bj3HRuQ7RM+xEenKE8+xiVm8ySXjbJl/o0vG8Waq2IiIiLiZ3hXlOUpsRERE3Exv9/YcLR4WERERr6GKjYiIiJupYOM5SmxERETcTK0oz1ErSkRERLyGKjYiIiJupoqN5yixERERcTPlNZ6jxEZERMTNVLHxHK2xEREREa+hio2IiIibqWDjOUpsRERE3EytKM9RK0pERES8hio2IiIibqaCjecosREREXGzXspsPEatKBEREfEaqtiIiIi4mQo2nqPERkRExM20K8pzlNiIiIi4WS/lNR6jNTYiIiLiNVSxERERcTO1ojxHiY2IiIibKa/xHLWiRERExGuoYiMiIuJmJlSy8RQlNiIiIm6mXVGeo1aUiIiIeA1VbERERNxMu6I8R4mNiIiImymv8Ry1okRERMRrqGIjIiLiZr1UsvEYJTYiIiJuprzGc9SKEhERcTOTyeSSozsWLFjQ5ffDwsKM6w6HgwULFmC1WvH392fcuHHs2LHDaQy73c79999PcHAwAQEBJCUlUVdX5xTT3NxMSkoKFosFi8VCSkoKBw8edIqpra1l8uTJBAQEEBwczJw5c2hvb3eK2b59O7Gxsfj7+zN48GAWLlyIw+Ho1mcGJTYiIiJe66qrrqKhocE4tm/fblxbsmQJeXl5FBQUUFFRQVhYGHFxcRw6dMiIycjIoKioiMLCQsrLyzl8+DCJiYl0dnYaMcnJyVRXV1NcXExxcTHV1dWkpKQY1zs7O5k0aRJtbW2Ul5dTWFjIunXryMrKMmJaW1uJi4vDarVSUVFBfn4+S5cuJS8vr9ufWa0oERERN3NVK8put2O3253Omc1mzGbzSeN79+7tVKU5zuFw8OSTT/LQQw/xwx/+EICXXnqJ0NBQXn75ZWbNmkVLSwvPP/88q1atYsKECQCsXr2aIUOG8NZbb5GQkMDOnTspLi7GZrMRHR0NwIoVK4iJieEf//gHw4cPp6SkhA8//JC9e/ditVoB+O1vf0tqaiqPP/44gYGBrFmzhi+//JKVK1diNpuJiIjgo48+Ii8vj8zMzG5Vq1SxERERcbNeJpNLjtzcXKPlc/zIzc095X0//vhjrFYrw4YN4yc/+QmffvopALt376axsZH4+Hgj1mw2Exsby+bNmwGorKyko6PDKcZqtRIREWHEbNmyBYvFYiQ1AKNHj8ZisTjFREREGEkNQEJCAna7ncrKSiMmNjbWKUFLSEigvr6ePXv2dO+77la0iIiI9Jjs7GxaWlqcjuzs7JPGRkdH88c//pE333yTFStW0NjYyJgxY/j8889pbGwEIDQ01Ol3QkNDjWuNjY34+fkRFBR02piQkJAu9w4JCXGKOfE+QUFB+Pn5nTbm+M/HY86WWlEiIiJu5qpNUadrO53olltuMf565MiRxMTEcNlll/HSSy8xevToY/M6ocXjcDjO2PY5MeZk8a6IOb5wuLuLplWxERERcbOe2BV1ooCAAEaOHMnHH39srLs5sRrS1NRkVErCwsJob2+nubn5tDH79u3rcq/9+/c7xZx4n+bmZjo6Ok4b09TUBHStKp2JEhsREZHvALvdzs6dOxk0aBDDhg0jLCyM0tJS43p7ezsbN25kzJgxAERGRuLr6+sU09DQQE1NjRETExNDS0sL27ZtM2K2bt1KS0uLU0xNTQ0NDQ1GTElJCWazmcjISCNm06ZNTlvAS0pKsFqtDB06tFufU4mNiIiIm/Uyuebojrlz57Jx40Z2797N1q1b+fGPf0xrayszZszAZDKRkZFBTk4ORUVF1NTUkJqaSt++fUlOTgbAYrEwc+ZMsrKyKCsro6qqiunTpzNy5Ehjl9SIESOYOHEiaWlp2Gw2bDYbaWlpJCYmMnz4cADi4+MJDw8nJSWFqqoqysrKmDt3LmlpaQQGBgLHtoybzWZSU1OpqamhqKiInJycbu+IAq2xERERcbueeLt3XV0dP/3pTzlw4AADBw5k9OjR2Gw2LrnkEgDmzZvHkSNHSE9Pp7m5mejoaEpKSujXr58xxrJly+jduzdTp07lyJEjjB8/npUrV+Lj42PErFmzhjlz5hi7p5KSkigoKDCu+/j48MYbb5Cens7YsWPx9/cnOTmZpUuXGjEWi4XS0lJmz55NVFQUQUFBZGZmkpmZ2e3PbXKcxWP9Xn311bMeMCkpqduTcLWBd67t6SmInJf2rpjW01MQOe/08cB/4k9f/YFLxlk9/RqXjOPNzupv55QpU85qMJPJ5PQ0QhEREdG7ojzprBKbo0ePunseIiIiXqsnWlHfVVpjIyIi4mbdXfgr5+6cEpu2tjY2btxIbW1tl7dzzpkzxyUTExEREemubic2VVVV3Hrrrfz73/+mra2N/v37c+DAAfr27UtISIgSGxERkROoFeU53X6Ozc9//nMmT57MF198gb+/Pzabjc8++4zIyEinrVsiIiJyjMlFh5xZtxOb6upqsrKy8PHxwcfHB7vdzpAhQ1iyZAm//OUv3TFHERERkbPS7cTG19fXKKmFhoZSW1sLHHu4zvG/FhERka/1MplccsiZdXuNzahRo3j//ff53ve+x0033cQjjzzCgQMHWLVqFSNHjnTHHEVERL7VlJN4TrcrNjk5OQwaNAiA3/zmNwwYMIB7772XpqYmnnvuOZdPUERERORsdbtiExUVZfz1wIEDWb9+vUsnJCIi4m20K8pz9IA+ERERN1Ne4zndTmyGDRt22szz008//UYTEhERETlX3U5sMjIynH7u6OigqqqK4uJiHnzwQVfNS0RExGtoR5PndDuxeeCBB056/qmnnuL999//xhMSERHxNsprPKfbu6JO5ZZbbmHdunWuGk5ERMRrmEwmlxxyZi5LbP7nf/6H/v37u2o4ERERkW47pwf0/WfW6HA4aGxsZP/+/Tz99NMundy52rtiWk9PQeS8FHT9fT09BZHzzpGqArffw2VVBDmjbic2t912m1Ni06tXLwYOHMi4ceO48sorXTo5ERERb6A2kud0O7FZsGCBG6YhIiIi8s11uzrm4+NDU1NTl/Off/45Pj4+LpmUiIiIN+llcs0hZ9btio3D4Tjpebvdjp+f3zeekIiIiLdRUuI5Z53Y/P73vweO9Qn/8Ic/cMEFFxjXOjs72bRpk9bYiIiISI8668Rm2bJlwLGKzTPPPOPUdvLz82Po0KE888wzrp+hiIjIt5wWD3vOWSc2u3fvBuCmm27ir3/9K0FBQW6blIiIiDdRK8pzur3G5m9/+5s75iEiIiLyjXV7V9SPf/xjFi1a1OX8E088we233+6SSYmIiHgTk8k1h5xZtxObjRs3MmnSpC7nJ06cyKZNm1wyKREREW/Sy2RyySFn1u1W1OHDh0+6rdvX15fW1laXTEpERMSb6JUKntPt7zoiIoK1a9d2OV9YWEh4eLhLJiUiIiJyLrqd2Dz88MP85je/YcaMGbz00ku89NJL/OxnP+Oxxx7j4YcfdsccRUREvtXOhzU2ubm5mEwmMjIyjHMOh4MFCxZgtVrx9/dn3Lhx7Nixw+n37HY7999/P8HBwQQEBJCUlERdXZ1TTHNzMykpKVgsFiwWCykpKRw8eNAppra2lsmTJxMQEEBwcDBz5syhvb3dKWb79u3Exsbi7+/P4MGDWbhw4SkfDHwq3U5skpKSeOWVV/jkk09IT08nKyuLf/3rX7z99tsMHTq0u8OJiIh4vZ5eY1NRUcFzzz3H1Vdf7XR+yZIl5OXlUVBQQEVFBWFhYcTFxXHo0CEjJiMjg6KiIgoLCykvL+fw4cMkJibS2dlpxCQnJ1NdXU1xcTHFxcVUV1eTkpJiXO/s7GTSpEm0tbVRXl5OYWEh69atIysry4hpbW0lLi4Oq9VKRUUF+fn5LF26lLy8vG59VpOju6nQCQ4ePMiaNWt4/vnn+eCDD5w+aE/58quenoHI+Sno+vt6egoi550jVQVuv8fDxR+7ZJzfTLyi279z+PBhrrvuOp5++mkee+wxrr32Wp588kkcDgdWq5WMjAzmz58PHKvOhIaGsnjxYmbNmkVLSwsDBw5k1apVTJs2DYD6+nqGDBnC+vXrSUhIYOfOnYSHh2Oz2YiOjgbAZrMRExPDrl27GD58OBs2bCAxMZG9e/ditVqBY0tYUlNTaWpqIjAwkOXLl5Odnc2+ffswm80ALFq0iPz8fOrq6s76IYfnvJ7p7bffZvr06VitVgoKCrj11lt5//33z3U4ERERr+WqVpTdbqe1tdXpsNvtp7337NmzmTRpEhMmTHA6v3v3bhobG4mPjzfOmc1mYmNj2bx5MwCVlZV0dHQ4xVitViIiIoyYLVu2YLFYjKQGYPTo0VgsFqeYiIgII6kBSEhIwG63U1lZacTExsYaSc3xmPr6evbs2XPW33W3Epu6ujoee+wxLr30Un76058SFBRER0cH69at47HHHmPUqFHdGU5EROQ7wVVv987NzTXWsRw/cnNzT3nfwsJC/u///u+kMY2NjQCEhoY6nQ8NDTWuNTY24ufn1+VtAyfGhISEdBk/JCTEKebE+wQFBeHn53famOM/H485G2ed2Nx6662Eh4fz4Ycfkp+fT319Pfn5+Wd9IxEREflmsrOzaWlpcTqys7NPGrt3714eeOABVq9eTZ8+fU455oktHofDcca2z4kxJ4t3Rczx1TLdedfWWT/HpqSkhDlz5nDvvfdyxRXd7/GJiIh8V7nq4Xpms9mpVXM6lZWVNDU1ERkZaZzr7Oxk06ZNFBQU8I9//AM4Vg0ZNGiQEdPU1GRUSsLCwmhvb6e5udmpatPU1MSYMWOMmH379nW5//79+53G2bp1q9P15uZmOjo6nGJOrMw0NTUBXatKp3PWFZt3332XQ4cOERUVRXR0NAUFBezfv/+sbyQiIvJd1RPbvcePH8/27duprq42jqioKO644w6qq6u59NJLCQsLo7S01Pid9vZ2Nm7caCQtkZGR+Pr6OsU0NDRQU1NjxMTExNDS0sK2bduMmK1bt9LS0uIUU1NTQ0NDgxFTUlKC2Ww2Eq+YmBg2bdrktAW8pKQEq9XarV3XZ53YxMTEsGLFChoaGpg1axaFhYUMHjyYo0ePUlpa6rQ1TERERHpWv379iIiIcDoCAgIYMGAAERERxjNtcnJyKCoqoqamhtTUVPr27UtycjIAFouFmTNnkpWVRVlZGVVVVUyfPp2RI0cai5FHjBjBxIkTSUtLw2azYbPZSEtLIzExkeHDhwMQHx9PeHg4KSkpVFVVUVZWxty5c0lLSyMwMBA4tmXcbDaTmppKTU0NRUVF5OTkkJmZ2a1WVLd3RfXt25e77rqL8vJytm/fTlZWFosWLSIkJISkpKTuDiciIuL1XLV42NXmzZtHRkYG6enpREVF8a9//YuSkhL69etnxCxbtowpU6YwdepUxo4dS9++fXnttdfw8fExYtasWcPIkSOJj48nPj6eq6++mlWrVhnXfXx8eOONN+jTpw9jx45l6tSpTJkyhaVLlxoxFouF0tJS6urqiIqKIj09nczMTDIzM7v1mb7xc2zgWM/utdde44UXXuDVV1/9psN9Y3qOjcjJ6Tk2Il154jk2OWX/dMk4vxx/mUvG8Wbdfgnmyfj4+DBlyhSmTJniiuFERES8ijuqLXJyeuGoiIiIeA2XVGxERETk1FSx8RwlNiIiIm7WnV098s2oFSUiIiJeQxUbERERN1MrynOU2IiIiLiZOlGeo1aUiIiIeA1VbERERNzMVS/BlDNTYiMiIuJmWmPjOWpFiYiIiNdQxUZERMTN1InyHCU2IiIibtYLZTaeosRGRETEzVSx8RytsRERERGvoYqNiIiIm2lXlOcosREREXEzPcfGc9SKEhEREa+hio2IiIibqWDjOUpsRERE3EytKM9RK0pERES8hio2IiIibqaCjecosREREXEztUc8R9+1iIiIeA1VbERERNzMpF6UxyixERERcTOlNZ6jxEZERMTNtN3bc7TGRkRERLyGKjYiIiJupnqN5yixERERcTN1ojxHrSgRERHxGkpsRERE3MxkMrnk6I7ly5dz9dVXExgYSGBgIDExMWzYsMG47nA4WLBgAVarFX9/f8aNG8eOHTucxrDb7dx///0EBwcTEBBAUlISdXV1TjHNzc2kpKRgsViwWCykpKRw8OBBp5ja2lomT55MQEAAwcHBzJkzh/b2dqeY7du3Exsbi7+/P4MHD2bhwoU4HI5ufWZQYiMiIuJ2vVx0dMdFF13EokWLeP/993n//fe5+eabue2224zkZcmSJeTl5VFQUEBFRQVhYWHExcVx6NAhY4yMjAyKioooLCykvLycw4cPk5iYSGdnpxGTnJxMdXU1xcXFFBcXU11dTUpKinG9s7OTSZMm0dbWRnl5OYWFhaxbt46srCwjprW1lbi4OKxWKxUVFeTn57N06VLy8vK6+anB5DiXdOg89+VXPT0DkfNT0PX39fQURM47R6oK3H6PtVX/csk400YN/ka/379/f5544gnuuusurFYrGRkZzJ8/HzhWnQkNDWXx4sXMmjWLlpYWBg4cyKpVq5g2bRoA9fX1DBkyhPXr15OQkMDOnTsJDw/HZrMRHR0NgM1mIyYmhl27djF8+HA2bNhAYmIie/fuxWq1AlBYWEhqaipNTU0EBgayfPlysrOz2bdvH2azGYBFixaRn59PXV1dt6pVqtiIiIi4mataUXa7ndbWVqfDbref8f6dnZ0UFhbS1tZGTEwMu3fvprGxkfj4eCPGbDYTGxvL5s2bAaisrKSjo8Mpxmq1EhERYcRs2bIFi8ViJDUAo0ePxmKxOMVEREQYSQ1AQkICdrudyspKIyY2NtZIao7H1NfXs2fPnm5910psRERE3MzkoiM3N9dYy3L8yM3NPeV9t2/fzgUXXIDZbOaee+6hqKiI8PBwGhsbAQgNDXWKDw0NNa41Njbi5+dHUFDQaWNCQkK63DckJMQp5sT7BAUF4efnd9qY4z8fjzlb2u4tIiLyLZGdnU1mZqbTuf+scpxo+PDhVFdXc/DgQdatW8eMGTPYuHGjcf3EFo/D4Thj2+fEmJPFuyLm+EqZ7i6aVsVGRETEzVzVijKbzcYup+PH6RIbPz8/Lr/8cqKiosjNzeWaa67hd7/7HWFhYUDXakhTU5NRKQkLC6O9vZ3m5ubTxuzbt6/Lfffv3+8Uc+J9mpub6ejoOG1MU1MT0LWqdCZKbERERNysJ3ZFnYzD4cButzNs2DDCwsIoLS01rrW3t7Nx40bGjBkDQGRkJL6+vk4xDQ0N1NTUGDExMTG0tLSwbds2I2br1q20tLQ4xdTU1NDQ0GDElJSUYDabiYyMNGI2bdrktAW8pKQEq9XK0KFDu/UZldiIiIi4WU88x+aXv/wl7777Lnv27GH79u089NBDvPPOO9xxxx2YTCYyMjLIycmhqKiImpoaUlNT6du3L8nJyQBYLBZmzpxJVlYWZWVlVFVVMX36dEaOHMmECRMAGDFiBBMnTiQtLQ2bzYbNZiMtLY3ExESGDx8OQHx8POHh4aSkpFBVVUVZWRlz584lLS2NwMBA4NiWcbPZTGpqKjU1NRQVFZGTk0NmZma3P7fW2IiIiHihffv2kZKSQkNDAxaLhauvvpri4mLi4uIAmDdvHkeOHCE9PZ3m5maio6MpKSmhX79+xhjLli2jd+/eTJ06lSNHjjB+/HhWrlyJj4+PEbNmzRrmzJlj7J5KSkqioODrLfQ+Pj688cYbpKenM3bsWPz9/UlOTmbp0qVGjMViobS0lNmzZxMVFUVQUBCZmZld1hOdDT3HRuQ7RM+xEenKE8+xeeXv3dvZcypTrg5zyTjeTBUbERERN9NLMD1Ha2xERETEa6hiIyIi4ma9UMnGU5TYiIiIuJlaUZ6jVpSIiIh4DVVsRERE3MykVpTHKLERERFxM7WiPEetKBEREfEaqtiIiIi4mXZFeY4SGxERETdTK8pzlNiIiIi4mRIbz9EaGxEREfEaqtiIiIi4mbZ7e44SGxERETfrpbzGY9SKEhEREa+hio2IiIibqRXlOUpsRERE3Ey7ojxHrSgRERHxGqrYiIiIuJlaUZ6jxEZERMTNtCvKc9SKEhEREa+hio24zdo/rWHli89zYP9+Lrv8Cub94pdcFxnV09MSOS0fn178atat/OTWKEIHBNJ4oJVVr9lYtOJNHA4HAAH+fjw25zYm33Q1/S0BfFb/BU8XvsOKv5Qb49z1w7FMuyWKa6+8iMAL/An7wYO0HD5iXP9B5BWU/OGBk87hhjuWUPlhLQCR4Rfzmzm3MSp8CA4HVO74jIeefIW/f/Qvp9/JSBnPXT8ay8WDgtj/xWGe+8u7PPFCiau/HjlHakV5jhIbcYviDetZsiiXhx5+lGtHXcf//LmQ9FlpFL36BoOs1p6ensgpZaXGcfePbyDtkVV8+M8GIq+6mGcXTKf10Jc89ad3AFgy90fERn2POx/6I5/Vf86EmBH8LnsqDftbeP2d7QD07eNL6eYPKd38Ib+Zc1uX+9g++JShE7Kdzj2SnsjN0cONpOaCvmZefXo2r7+znQdy19LbpxcP3zuJV5+ezeUTf8VXXx0F4Lfzfsz40VeSvayImo/rsVzQhwFBF7jxW5Lu0q4oz1FiI26x6qUX+a8f/Ygf/vh2AOZlP8TmzeX8ee2feODnWT08O5FTi756GK9v/DvF5TsAqG34gqkTo7gu/GKnmNWvb+Xdyo8BeOGv7zHzR2O5LvxiI7EpePkd4Fhl5mQ6vupk3+eHjJ979+7FpNiRPLN2k3Hue0ND6W8J4DfLX6du30EAHn92A+//5ZcMCevP7roDDB8WStqPf0Dk7Y/z8WdNLvsexLWU13iO1tiIy3W0t7Pzwx3EjLnB6XzMmLF8UF3VQ7MSOTtbqv/JTd8fzuUXhwAw8nuDibn2Ut58b4cRs7n6UxJjR2IdaAHgxqgruOKSEN7avPOc75sYezXBF17A6ldtxrmP9uxjf/MhZkwZg29vH/qYfUmdEsOOT+qpbfgCgEk3jmT3vw5w640R7Hx9Abve+DVPP5JMUGDfc56LyLfZt75iY7fbsdvtTuccPmbMZnMPzUiaDzbT2dnJgAEDnM4PGBDMgQP7e2hWImdn6YulBF7gzwdFv6Kz04GPj4lHn3qdPxdXGjFZi//C048k88+Sx+no6OSo4yj3LnyZzdWfnvN9Z0yJoXTLTqMyA3D433YS7v4df1k2i+y0iQB8/FkTSbOforPzWBtq6EXBXDyoPz+cMIq7H15Fr169WDL3h7z8xExumZV/zvMR1+qlXpTHnNcVm71793LXXXedNiY3NxeLxeJ0PLE410MzlNMxnfAH2eFwdDkncr65PSGSn956Pam/fImY5MXc/cgqMlLGc8fkaCNm9k/H8f2RQ/nRA88w5o7F/CKviN9lT+Om6OHndM/BIRcSFzOCl17Z4nS+j9mXZxdMZ8sHnxL7s6XcfGceOz9toCj/XvqYfYFj/8LsY/Zl5sOreK/qn7xb+TH3/noN474/nCsuCTn3L0JcyuSiQ87svK7YfPHFF7z00ku88MILp4zJzs4mMzPT6ZzDR9WanhR0YRA+Pj4cOHDA6fwXX3zOgAHBPTQrkbOTkzGFpS+W8pc3j1VodnxSz8WD+vPgnXGseW0rfcy+/Pr+yUzLXGGsw6n5uJ6rh19ERsp4/rb1H92+Z8pto/m8pY3XN/7d6fy0W6K42Nqf2Bm/NXZkzcheScOmJUwedzV/ebOSxgMtdHR08knt1+trdu3eB8CQsP5adyPfOT2a2Lz66qunvf7pp2cu65rNXdtOX371jaYl35Cvnx8jwq/Ctvk9xk+IM87bNm9m3M3je3BmImfm38ePo46jTuc6jzro1etYgdu3tw9+vr05+v8TDSOm8yi9zvEpbD9LGs3Lr28zdjkd17ePH0ePOoykBuCow4HD8XVrY0v1p/j6+jDsomB21x37j4njlZrj63DkPKByi8f0aGIzZcoUTCaT0x/aE6l18e2UMuNOHvrFPMIjIrjmmlGs+8taGhoauH3aT3p6aiKntX7TdubPTGBvQzMf/rOBa6+8iDnTb+KPrxxb1Huo7Us2vf8xORlTOPJlB7UNX/CDyMu5I/H7zM/7qzFO6IB+hA4I5LKLj1UpI66wcqjtS/Y2NtPc+m8jbtz3v8ewi4JZ+crmLnMps+0iJ2MKT2ZPZXnhRnqZTMy9M56vOjvZ+P5HALy99R/834e1PLvgDh58Yh29epl48hdTeWvLTqcqjvQsPcfGc0yO02UVbjZ48GCeeuoppkyZctLr1dXVREZG0tnZ2a1xVbE5P6z90xpWvvA8+/c3cfkV3+PB+dlERl3f09P6Tgu6/r6ensJ574K+Zh5NTyTp5msYGHQBDftb+HNxJTnPbaDjq2P/LAod0I+F99/GhJgrCQrsS23DF7zw1838fvXbxjgPzbqVX91za5fx0x5ZxerXtho/r8xJ5eJBQdx857KTzufm6Ct5aNYthF8+iKNHHXywq44FT73Gtu17jJhBAy3kzb+d8aOvpO1IOyXvfcgv8v7qlEDJqR2pKnD7Pbb+s8Ul40RfZnHJON6sRxObpKQkrr32WhYuXHjS6x988AGjRo3i6NGjJ71+KkpsRE5OiY1IV55IbLZ96prE5vuXnn1ik5uby1//+ld27dqFv78/Y8aMYfHixQwf/vUid4fDwa9//Wuee+45mpubiY6O5qmnnuKqq64yYux2O3PnzuVPf/oTR44cYfz48Tz99NNcdNFFRkxzczNz5swxlpgkJSWRn5/PhRdeaMTU1tYye/Zs3n77bfz9/UlOTmbp0qX4+fkZMdu3b+e+++5j27Zt9O/fn1mzZvHwww93q3vTo7uiHnzwQcaMGXPK65dffjl/+9vfPDgjERER1+uJXVEbN25k9uzZ2Gw2SktL+eqrr4iPj6etrc2IWbJkCXl5eRQUFFBRUUFYWBhxcXEcOvT1wyMzMjIoKiqisLCQ8vJyDh8+TGJiolM3JTk5merqaoqLiykuLqa6upqUlBTjemdnJ5MmTaKtrY3y8nIKCwtZt24dWVlfP7C1tbWVuLg4rFYrFRUV5Ofns3TpUvLy8rr1uXu0YuMuqtiInJwqNiJdeaJiU+Giis313ajYnGj//v2EhISwceNGbrzxRhwOB1arlYyMDObPnw8cq86EhoayePFiZs2aRUtLCwMHDmTVqlVMmzYNgPr6eoYMGcL69etJSEhg586dhIeHY7PZiI4+9lgEm81GTEwMu3btYvjw4WzYsIHExET27t2L9f+/VqewsJDU1FSampoIDAxk+fLlZGdns2/fPmNT0KJFi8jPz6euru6sqzbn9XNsREREvIKLSjZ2u53W1lan48SH1J5KS8ux5Kp///4A7N69m8bGRuLj440Ys9lMbGwsmzcfW8xeWVlJR0eHU4zVaiUiIsKI2bJlCxaLxUhqAEaPHo3FYnGKiYiIMJIagISEBOx2O5WVlUZMbGys007nhIQE6uvr2bNnz1l9RlBiIyIi4nYmF/3vZA+lzc0980NpHQ4HmZmZ3HDDDURERADQ2NgIQGhoqFNsaGioca2xsRE/Pz+CgoJOGxMS0vVhkCEhIU4xJ94nKCgIPz+/08Yc//l4zNk4rx/QJyIi4g1c9eSSkz2U9mxeIXTffffx97//nfLy8pPMrftPiT8x5mTxrog5vlrmW7N4WERERM6e2WwmMDDQ6ThTYnP//ffz6quv8re//c1pJ1NYWBjQtRrS1NRkVErCwsJob2+nubn5tDH79u3rct/9+/c7xZx4n+bmZjo6Ok4b09R07FlMJ1ZyTkeJjYiIiJv1xK4oh8PBfffdx1//+lfefvtthg0b5nR92LBhhIWFUVpaapxrb29n48aNxo7lyMhIfH19nWIaGhqoqakxYmJiYmhpaWHbtm1GzNatW2lpaXGKqampoaGhwYgpKSnBbDYTGRlpxGzatIn29nanGKvVytChQ8/6cyuxERERcbceyGxmz57N6tWrefnll+nXrx+NjY00NjZy5MiRY1MymcjIyCAnJ4eioiJqampITU2lb9++JCcnA2CxWJg5cyZZWVmUlZVRVVXF9OnTGTlyJBMmTABgxIgRTJw4kbS0NGw2GzabjbS0NBITE41n5sTHxxMeHk5KSgpVVVWUlZUxd+5c0tLSCAwMBI5tGTebzaSmplJTU0NRURE5OTlkZmZ2qxWlNTYiIiJeaPny5QCMGzfO6fyLL75IamoqAPPmzePIkSOkp6cbD+grKSmhX79+RvyyZcvo3bs3U6dONR7Qt3LlSnx8fIyYNWvWMGfOHGP3VFJSEgUFX2+j9/Hx4Y033iA9PZ2xY8c6PaDvOIvFQmlpKbNnzyYqKoqgoCAyMzO7rCk6Ez3HRuQ7RM+xEenKE8+xqfrs0JmDzsKoS/qdOeg7ThUbERERN9P7nD1Ha2xERETEa6hiIyIi4mYq2HiOEhsRERF3U2bjMWpFiYiIiNdQxUZERMTNTCrZeIwSGxERETfTrijPUWIjIiLiZsprPEdrbERERMRrqGIjIiLibirZeIwSGxERETfT4mHPUStKREREvIYqNiIiIm6mXVGeo8RGRETEzZTXeI5aUSIiIuI1VLERERFxN5VsPEaJjYiIiJtpV5TnqBUlIiIiXkMVGxERETfTrijPUWIjIiLiZsprPEeJjYiIiLsps/EYrbERERERr6GKjYiIiJtpV5TnKLERERFxMy0e9hy1okRERMRrqGIjIiLiZirYeI4SGxEREXdTZuMxakWJiIiI11DFRkRExM20K8pzlNiIiIi4mXZFeY5aUSIiIuI1lNiIiIi4mclFR3dt2rSJyZMnY7VaMZlMvPLKK07XHQ4HCxYswGq14u/vz7hx49ixY4dTjN1u5/777yc4OJiAgACSkpKoq6tzimlubiYlJQWLxYLFYiElJYWDBw86xdTW1jJ58mQCAgIIDg5mzpw5tLe3O8Vs376d2NhY/P39GTx4MAsXLsThcHTrMyuxERERcbceymza2tq45pprKCgoOOn1JUuWkJeXR0FBARUVFYSFhREXF8ehQ4eMmIyMDIqKiigsLKS8vJzDhw+TmJhIZ2enEZOcnEx1dTXFxcUUFxdTXV1NSkqKcb2zs5NJkybR1tZGeXk5hYWFrFu3jqysLCOmtbWVuLg4rFYrFRUV5Ofns3TpUvLy8rr1mU2O7qZC3wJfftXTMxA5PwVdf19PT0HkvHOk6uT/0nelzz63u2ScSwaYz/l3TSYTRUVFTJkyBThWrbFarWRkZDB//nzgWHUmNDSUxYsXM2vWLFpaWhg4cCCrVq1i2rRpANTX1zNkyBDWr19PQkICO3fuJDw8HJvNRnR0NAA2m42YmBh27drF8OHD2bBhA4mJiezduxer1QpAYWEhqampNDU1ERgYyPLly8nOzmbfvn2Yzcc+56JFi8jPz6eurg7TWS5UUsVGRETkW8Jut9Pa2up02O3nljTt3r2bxsZG4uPjjXNms5nY2Fg2b94MQGVlJR0dHU4xVquViIgII2bLli1YLBYjqQEYPXo0FovFKSYiIsJIagASEhKw2+1UVlYaMbGxsUZSczymvr6ePXv2nPXnUmIjIiLiZiaTa47c3FxjHcvxIzc395zm1NjYCEBoaKjT+dDQUONaY2Mjfn5+BAUFnTYmJCSky/ghISFOMSfeJygoCD8/v9PGHP/5eMzZ0HZvERERN3PVbu/s7GwyMzOdzv1nheNcnNjicTgcZ2z7nBhzsnhXxBxfLXO2bShQxUZERORbw2w2ExgY6HSca2ITFhYGdK2GNDU1GZWSsLAw2tvbaW5uPm3Mvn37uoy/f/9+p5gT79Pc3ExHR8dpY5qamoCuVaXTUWIjIiLiZq5qRbnSsGHDCAsLo7S01DjX3t7Oxo0bGTNmDACRkZH4+vo6xTQ0NFBTU2PExMTE0NLSwrZt24yYrVu30tLS4hRTU1NDQ0ODEVNSUoLZbCYyMtKI2bRpk9MW8JKSEqxWK0OHDj3rz6XERkRExO16Zr/34cOHqa6uprq6Gji2YLi6upra2lpMJhMZGRnk5ORQVFRETU0Nqamp9O3bl+TkZAAsFgszZ84kKyuLsrIyqqqqmD59OiNHjmTChAkAjBgxgokTJ5KWlobNZsNms5GWlkZiYiLDhw8HID4+nvDwcFJSUqiqqqKsrIy5c+eSlpZGYGAgcGzLuNlsJjU1lZqaGoqKisjJySEzM7NbrSitsREREfFS77//PjfddJPx8/H1OTNmzGDlypXMmzePI0eOkJ6eTnNzM9HR0ZSUlNCvXz/jd5YtW0bv3r2ZOnUqR44cYfz48axcuRIfHx8jZs2aNcyZM8fYPZWUlOT07BwfHx/eeOMN0tPTGTt2LP7+/iQnJ7N06VIjxmKxUFpayuzZs4mKiiIoKIjMzMwua4rORM+xEfkO0XNsRLryxHNs/nWw/cxBZ2HwhX4uGcebqWIjIiLiZnoHpudojY2IiIh4DVVsRERE3MzVO5rk1JTYiIiIuJlJzSiPUWIjIiLibsprPEZrbERERMRrqGIjIiLiZirYeI4SGxERETfT4mHPUStKREREvIYqNiIiIm6mXVGeo8RGRETE3ZTXeIxaUSIiIuI1VLERERFxMxVsPEeJjYiIiJtpV5TnqBUlIiIiXkMVGxERETfTrijPUWIjIiLiZmpFeY5aUSIiIuI1lNiIiIiI11ArSkRExM3UivIcJTYiIiJupsXDnqNWlIiIiHgNVWxERETcTK0oz1FiIyIi4mbKazxHrSgRERHxGqrYiIiIuJtKNh6jxEZERMTNtCvKc9SKEhEREa+hio2IiIibaVeU5yixERERcTPlNZ6jxEZERMTdlNl4jNbYiIiIeLGnn36aYcOG0adPHyIjI3n33Xd7ekpupcRGRETEzUwu+l93rV27loyMDB566CGqqqr4wQ9+wC233EJtba0bPuX5weRwOBw9PQlX+/Krnp6ByPkp6Pr7enoKIuedI1UFbr+Hq/691KebC0iio6O57rrrWL58uXFuxIgRTJkyhdzcXNdM6jyjio2IiMi3hN1up7W11emw2+0njW1vb6eyspL4+Hin8/Hx8WzevNkT0+0RXrl4uLsZrbiH3W4nNzeX7OxszGZzT09H8Mx/mcqZ6c/Gd4+r/r204LFcfv3rXzude/TRR1mwYEGX2AMHDtDZ2UloaKjT+dDQUBobG10zofOQV7ai5PzQ2tqKxWKhpaWFwMDAnp6OyHlDfzbkXNnt9i4VGrPZfNIEub6+nsGDB7N582ZiYmKM848//jirVq1i165dbp9vT1BtQ0RE5FviVEnMyQQHB+Pj49OlOtPU1NSliuNNtMZGRETEC/n5+REZGUlpaanT+dLSUsaMGdNDs3I/VWxERES8VGZmJikpKURFRRETE8Nzzz1HbW0t99xzT09PzW2U2IjbmM1mHn30US2OFDmB/myIp0ybNo3PP/+chQsX0tDQQEREBOvXr+eSSy7p6am5jRYPi4iIiNfQGhsRERHxGkpsRERExGsosRERERGvocRGREREvIYSG3Gbp59+mmHDhtGnTx8iIyN59913e3pKIj1q06ZNTJ48GavVislk4pVXXunpKYl4HSU24hZr164lIyODhx56iKqqKn7wgx9wyy23UFtb29NTE+kxbW1tXHPNNRQU6J1dIu6i7d7iFtHR0Vx33XUsX77cODdixAimTJlCbm5uD85M5PxgMpkoKipiypQpPT0VEa+iio24XHt7O5WVlcTHxzudj4+PZ/PmzT00KxER+S5QYiMud+DAATo7O7u8ZC00NLTLy9hERERcSYmNuI3JZHL62eFwdDknIiLiSkpsxOWCg4Px8fHpUp1pamrqUsURERFxJSU24nJ+fn5ERkZSWlrqdL60tJQxY8b00KxEROS7QG/3FrfIzMwkJSWFqKgoYmJieO6556itreWee+7p6amJ9JjDhw/zySefGD/v3r2b6upq+vfvz8UXX9yDMxPxHtruLW7z9NNPs2TJEhoaGoiIiGDZsmXceOONPT0tkR7zzjvvcNNNN3U5P2PGDFauXOn5CYl4ISU2IiIi4jW0xkZERES8hhIbERER8RpKbERERMRrKLERERERr6HERkRERLyGEhsRERHxGkpsRERExGsosRERERGvocRGxAstWLCAa6+91vg5NTWVKVOmeHwee/bswWQyUV1d7fF7i8h3kxIbEQ9KTU3FZDJhMpnw9fXl0ksvZe7cubS1tbn1vr/73e/O+pH9SkZE5NtML8EU8bCJEyfy4osv0tHRwbvvvsvdd99NW1sby5cvd4rr6OjA19fXJfe0WCwuGUdE5Hynio2Ih5nNZsLCwhgyZAjJycnccccdvPLKK0b76IUXXuDSSy/FbDbjcDhoaWnhv//7vwkJCSEwMJCbb76ZDz74wGnMRYsWERoaSr9+/Zg5cyZffvml0/UTW1FHjx5l8eLFXH755ZjNZi6++GIef/xxAIYNGwbAqFGjMJlMjBs3zvi9F198kREjRtCnTx+uvPJKnn76aaf7bNu2jVGjRtGnTx+ioqKoqqpy4TcnInJmqtiI9DB/f386OjoA+OSTT/jzn//MunXr8PHxAWDSpEn079+f9evXY7FYePbZZxk/fjwfffQR/fv3589//jOPPvooTz31FD/4wQ9YtWoVv//977n00ktPec/s7GxWrFjBsmXLuOGGG2hoaGDXrl3AseTk+9//Pm+99RZXXXUVfn5+AKxYsYJHH32UgoICRo0aRVVVFWlpaQQEBDBjxgza2tpITEzk5ptvZvXq1ezevZsHHnjAzd+eiMgJHCLiMTNmzHDcdtttxs9bt251DBgwwDF16lTHo48+6vD19XU0NTUZ18vKyhyBgYGOL7/80mmcyy67zPHss886HA6HIyYmxnHPPfc4XY+OjnZcc801J71va2urw2w2O1asWHHSOe7evdsBOKqqqpzODxkyxPHyyy87nfvNb37jiImJcTgcDsezzz7r6N+/v6Otrc24vnz58pOOJSLiLmpFiXjY66+/zgUXXECfPn2IiYnhxhtvJD8/H4BLLrmEgQMHGrGVlZUcPnyYAQMGcMEFFxjH7t27+ec//wnAzp07iYmJcbrHiT//p507d2K32xk/fvxZz3n//v3s3buXmTNnOs3jsccec5rHNddcQ9++fc9qHiIi7qBWlIiH3XTTTSxfvhxfX1+sVqvTAuGAgACn2KNHjzJo0CDeeeedLuNceOGF53R/f3//bv/O0aNHgWPtqOjoaKdrx1tmDofjnOYjIuJKSmxEPCwgIIDLL7/8rGKvu+46Ghsb6d27N0OHDj1pzIgRI7DZbPzsZz8zztlstlOOecUVV+Dv709ZWRl33313l+vH19R0dnYa50JDQxk8eDCffvopd9xxx0nHDQ8PZ9WqVRw5csRInk43DxERd1ArSuQ8NmHCBGJiYpgyZQpvvvkme/bsYfPmzfzqV7/i/fffB+CBBx7ghRde4IUXXuCjjz7i0UcfZceOHaccs0+fPsyfP5958+bxxz/+kX/+85/YbDaef/55AEJCQvD396e4uJh9+/bR0tICHHvoX25uLr/73e/46KOP2L59Oy+++CJ5eXkAJCcn06tXL2bOnMmHH37I+vXrWbp0qZu/IRERZ0psRM5jJpOJ9evXc+ONN3LXXXfxve99j5/85Cfs2bOH0NBQAKZNm8YjjzzC/PnziYyM5LPPPuPee+897bgPP/wwWVlZPPLII4wYMYJp06bR1NQEQO/evfn973/Ps88+i9Vq5bbbbgPg7rvv5g9/+AMrV65k5MiRxMbGsnLlSmN7+AUXXMBrr73Ghx9+yKhRo3jooYdYvHixG78dEZGuTA41xkVERMRLqGIjIiIiXkOJjYiIiHgNJTYiIiLiNZTYiIiIiNdQYiMiIiJeQ4mNiIiIeA0lNiIiIuI1lNiIiIiI11BiIyIiIl5DiY2IiIh4DSU2IiIi4jX+H8LoBcOPEdM2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(pandas_df['sentiment'], pandas_df['sentiment'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c603e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34b6d38a",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f00a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |words                                                                                                                             |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |[, , , a11, thats, a, bummer, , you, shoulda, got, david, carr, of, third, day, to, do, it, d]                                    |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|[is, upset, that, he, cant, update, his, facebook, by, texting, it11, and, might, cry, as, a, result, , school, today, also, blah]|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |[, i, dived, many, times, for, the, ball, managed, to, save, , , the, rest, go, out, of, bounds]                                  |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |[my, whole, body, feels, itchy, and, like, its, on, fire]                                                                         |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |[, no, its, not, behaving, at, all, im, mad, why, am, i, here, because, i, cant, see, you, all, over, there]                      |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tweets_df = tokenizer.transform(tweets_test)\n",
    "\n",
    "# Show the result\n",
    "tweets_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baff51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |text                                                                                                       |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|   a11 thats a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|1    |2009-04-07 05:19:49|is upset that he cant update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "|2    |2009-04-07 05:19:53| i dived many times for the ball managed to save   the rest go out of bounds                               |\n",
      "|3    |2009-04-07 05:19:57|my whole body feels itchy and like its on fire                                                             |\n",
      "|4    |2009-04-07 05:19:57| no its not behaving at all im mad why am i here because i cant see you all over there                     |\n",
      "+-----+-------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "tweets_test.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019887d",
   "metadata": {},
   "source": [
    "#### Sentiment extration using SparkNLP library/pipeline\n",
    "\n",
    "The sentiment analysis results will be stored in the 'sentiment.result' column once the data is preprocessed.\n",
    "\n",
    "This pipeline uses the DocumentAssembler to assemble the words into documents, which is required for the Spark NLP SentimentDetector. Then, it applies the sentiment analysis using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0f1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\r\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab5ec543-2156-45c8-9344-a00b26145bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "#pipeline = PretrainedPipeline(\"analyze_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c799443-2c64-42d6-b501-f7377412e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import lit\n",
    "\n",
    "# Assuming your existing DataFrame is named `tweets_df` and tokenized words are in the column \"words\"\n",
    "# Add a label column (e.g., 1 for positive, 0 for negative)\n",
    "#labeled_df = tweets_df.withColumn(\"label\", lit(1))  # possibvle to customize this based on sentiment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "965c8d98-2374-4434-9b96-dffcd93fe868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Combine tokenized words into a single column\n",
    "#labeled_df = labeled_df.withColumn(\"combined_text\", concat_ws(\" \", \"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c211d27-b532-4d70-be32-b770416596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.annotator import SentimentDetector\n",
    "\n",
    "# SentimentDetector setup\n",
    "#sentiment_detector = SentimentDetector() \\\n",
    " #   .setInputCols([\"combined_text\", \"words\"]) \\\n",
    "   # .setOutputCol(\"sentiment\")\n",
    "\n",
    "# Create a new pipeline with the SentimentDetector\n",
    "#pipeline_sentiment = Pipeline(stages=[sentiment_detector])\n",
    "\n",
    "# Apply sentiment detection\n",
    "#analyzed_df = pipeline_sentiment.fit(labeled_df).transform(labeled_df)\n",
    "\n",
    "# Show the results\n",
    "#analyzed_df.select(\"combined_text\", \"sentiment.result\", \"label\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b5b52-5493-4a36-bb5b-a732c0724926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9ac34-d1b6-4fce-8baf-ce9359af210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebd3fd-d8fc-496c-8354-f6bb4cd07dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc45b61-f87e-4184-a76d-a128d4fd634d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077b74-0eba-438f-a452-d5e639521d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7832770",
   "metadata": {},
   "source": [
    "transform these tokens into numerical feature vectors using HashingTF: Hashing Term Frequency. The output of this function is a sparse vector of term frequency counts for each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15bcb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeaaf2f",
   "metadata": {},
   "source": [
    "After obtaining this frequency vector, we need to pass it into the IDF function. This function will add weights to each word. The more frequent the word is, the lower its weight, and vice versa. This allows us to compensate for the bias in a large corpus of text like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf87c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98ecc2f0",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer instance\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a UDF to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "# Register the UDF\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply sentiment analysis to the DataFrame\n",
    "tweets_test = tweets_test.withColumn(\"sentiment_score\", analyze_sentiment_udf(\"text\"))\n",
    "\n",
    "# Show the result\n",
    "tweets_test.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a9008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8ce66135",
   "metadata": {},
   "source": [
    "# Broadcast the NLTK library to all worker nodes\n",
    "nltk.download('vader_lexicon')\n",
    "broadcast_nltk = spark.sparkContext.broadcast(nltk)\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer instance using the broadcasted NLTK\n",
    "sid = broadcast_nltk.value.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a UDF to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "# Register the UDF\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply sentiment analysis to the DataFrame\n",
    "tweets_test = tweets_test.withColumn(\"sentiment_score\", analyze_sentiment_udf(\"text\"))\n",
    "\n",
    "# Show the result\n",
    "tweets_test.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6da00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#error: externally-managed-environment\n",
    "\n",
    "#!pip3 install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ace4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#error: externally-managed-environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd7eb9",
   "metadata": {},
   "source": [
    "the main differences between the Pandas & PySpark, operations on Pyspark run faster than Pandas due to its distributed nature and parallel execution on multiple cores and machines.\n",
    "\n",
    "In other words, pandas run operations on a single node whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark processes operations many times faster than pandas.\n",
    "\n",
    "scaling a cluster involves a trade-off between performance and cost. Adding more resources may improve performance, but it also increases infrastructure costs. It's essential to find the right balance based on your application's requirements and budget constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989fa18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601fd3f0",
   "metadata": {},
   "source": [
    "The VADER sentiment analyzer is a simple rule-based model that works well for social media text. If you need more advanced sentiment analysis, you might want to explore machine learning-based approaches, such as using pre-trained models like BERT or spaCy.\n",
    "\n",
    "\"By far the most popular and comprehensive library, to my knowledge, for Spark-native distributed NLP, is spark-nlp from John Snow Labs. https://nlp.johnsnowlabs.com/ It is open source (but with commercial support options) and has a whole lot of functionality.\n",
    "\n",
    "You can also use spacy, nltk, and other non-Spark NLP libraries with Spark, by writing pandas UDFs that leverage these libraries, then applying them to data with Spark.\"https://community.databricks.com/t5/machine-learning/what-are-best-nlp-libraries-to-use-with-spark/td-p/24033#:~:text=You%20can%20also%20use%20spacy,them%20to%20data%20with%20Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397ba739",
   "metadata": {},
   "source": [
    "- store cleaned dataset back to hbase/mysql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb7845",
   "metadata": {},
   "source": [
    "#### Importing dataset from HBase using a Connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd392e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786231cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
