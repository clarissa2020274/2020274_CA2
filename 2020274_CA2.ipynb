{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0e1860",
   "metadata": {},
   "source": [
    "### MSc Data Analytics \n",
    "\n",
    "##### CA2 - Integrated Assignment sem2\n",
    "\n",
    "#### 2020274 - Clarissa Cardoso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This Notebook contains experimental features for CA2 using different databases to store and retreat files from. The goal of this project is to combine language processing techniques and a time series forecasting to predict the average sentiment of tweets for a certein period of time after the apropriate data cleaning and processing tecniques are applied.\n",
    "\n",
    "Fpr a better understanding the project will be divided into the following sections:\n",
    "\n",
    "- First section consists of importing dataset from various databases, and an attemp to evaluate their performance and usability, helping to select the most suitable dataset for the analysis.\n",
    "\n",
    "- Second section will focus on data cleaning and preprocessing the dataset\n",
    "\n",
    "- Third section focus on deeper EDA features and Natural Language Processing to undertand the dataset better prior to modeling and extract the sentiment from tweets given.\n",
    "\n",
    "- Section four centers on creating the time-series model and selection of apropriate parametrers and hyperparameters to run it.\n",
    "\n",
    "- Fifth section relies on training the model and validating/reacessing features that can be modified for better performance and compare model's results. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7da644",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "\n",
    "The goal of this project is to perform an analysis of the given dataset containing several tweets while experimenting with different databases to store data as well as creating a  time series forecast of the sentiment of the dataset. \n",
    "\n",
    "For the initial experimentation, after instalation of different noSQL databases as seen in class tutorials, I have decided to start with Hbase. One of the reasons why this was the first database used for the project is that it is built on top of HDFS as a part of Hadoop environment and provides a faster lookup on files while displaying lower latency for queries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f9e8f",
   "metadata": {},
   "source": [
    "### Libraries required for project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3ac7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries to deploy pyspark functions\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import count # Funcion to get the \"size\" of the data.\n",
    "from pyspark.sql.functions import when # When function.\n",
    "from pyspark.sql.functions import col # Function column.\n",
    "from pyspark.sql.functions import mean, min, max, stddev # Imports function for statistical features. \n",
    "from pyspark.sql import functions as F # Data processing framework.\n",
    "from pyspark.sql.functions import size, split # Imports function size and split.\n",
    "from pyspark.ml.feature import Tokenizer # Importing Tokenizer.\n",
    "from pyspark.sql.functions import regexp_replace # Remove / Replace function.\n",
    "from pyspark.sql.types import StructField, StructType # Importing features for Schema.\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType # Tools to create the schema.\n",
    "from pyspark.sql.functions import udf # Imports function UDF (user defined functions).\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import max as max_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b712d94",
   "metadata": {},
   "source": [
    "#### Importing dataset from HDFS\n",
    "\n",
    "My initial idea, once the dataset given was succesfuly stored in HDFS directory alocated for the CA development (\"CA2/ProjectTweets.csv\"), was to import it straighaway to a noSQL database and perform initial queries inside the HBase enviroment/shell to verify functionality.\n",
    "\n",
    "However my VM had continuous crashes during this process, and the HMaster node managed by Zookeeper kept showing slower times for initializing the commands. After a few seconds the Zookeeper Connection with HDFS and Hase nodes was lost and it was taking me a longer time span to find an alternative. Since the csv file was already in hadoop, I decided to first import from HDFS and the perform some initial cleaning and EDA using Spark framework to process the data to then store the cleneaded data back to HBAse through a connector between Pyspark and the database.\n",
    "\n",
    "\n",
    "- HDFS (Hadoop Distributed File System) is the primary storage system used by Hadoop applications. This open source framework works by rapidly transferring data between nodes. It's often used by companies who need to handle and store big data. <https://www.databricks.com/glossary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc6424",
   "metadata": {},
   "source": [
    "First step is to perform some basic exploratory data analysis to get a sense of the data. \n",
    "\n",
    "#### Check the first few rows of the dataset with .show()\n",
    "\n",
    "File was imported with a header marked as 'false' so pyspark will input labels insted of using the first row. This makes room to rename the labels in coming steps. set up schema as true so pyspark utilises the same scema present in the original file, without overlapping the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abb1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession including Legacy for timestamp\n",
    "spark = SparkSession.builder.appName(\"Test Tweets\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "# Define the file path in HDFS\n",
    "file_path = \"hdfs:///user/hduser/CA2/ProjectTweets.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "tweets_test = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame (optional)\n",
    "tweets_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e0566",
   "metadata": {},
   "source": [
    "#### Checking the schema of the dataset\n",
    "\n",
    "From this function we see most of the data is composed by strings, which makes sense, since we are working with mostly text. However, on the third column, with the dates of each tweet, we must have a datetime datatype in order to perform the timeseries analysis on further stages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a294330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5889696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|summary|               _c0|                 _c1|                 _c2|     _c3|                 _c4|                 _c5|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|1.9988175522956276E9|                null|    null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|1.9357607362267256E8|                null|    null|5.162733218454889E10|                null|\n",
      "|    min|                 0|          1467810369|Fri Apr 17 20:30:...|NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|           1599999|          2329205794|Wed May 27 07:27:...|NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get summary statistics\n",
    "tweets_test.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a2833",
   "metadata": {},
   "source": [
    "### Part I : Data Pre-Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59afc2",
   "metadata": {},
   "source": [
    "\n",
    "Rename cols and drop c1\n",
    "\n",
    "Convert the date column to a timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508dd527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|index|                date|query_flag|           user|                text|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "|    0|Mon Apr 06 22:19:...|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|Mon Apr 06 22:19:...|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|Mon Apr 06 22:19:...|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|Mon Apr 06 22:19:...|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|Mon Apr 06 22:19:...|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|    5|Mon Apr 06 22:20:...|  NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|Mon Apr 06 22:20:...|  NO_QUERY|        mybirch|         Need a hug |\n",
      "|    7|Mon Apr 06 22:20:...|  NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|Mon Apr 06 22:20:...|  NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|Mon Apr 06 22:20:...|  NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|   10|Mon Apr 06 22:20:...|  NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|   11|Mon Apr 06 22:20:...|  NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|   12|Mon Apr 06 22:20:...|  NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|   13|Mon Apr 06 22:20:...|  NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|Mon Apr 06 22:20:...|  NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|   15|Mon Apr 06 22:20:...|  NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|Mon Apr 06 22:20:...|  NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|   17|Mon Apr 06 22:20:...|  NO_QUERY|       armotley|about to file taxes |\n",
      "|   18|Mon Apr 06 22:20:...|  NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|Mon Apr 06 22:20:...|  NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+--------------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.drop(\"_c1\") \\\n",
    "           .withColumnRenamed(\"_c0\", \"index\") \\\n",
    "           .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "           .withColumnRenamed(\"_c3\", \"query_flag\") \\\n",
    "           .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "           .withColumnRenamed(\"_c5\", \"text\") \\\n",
    "\n",
    "tweets_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607de02",
   "metadata": {},
   "source": [
    "View a sample of the 'date' column, using the sample() function to double check the timezone used before conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e018e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                date|\n",
      "+--------------------+\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:20:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:22:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:23:...|\n",
      "|Mon Apr 06 22:25:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:26:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:27:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:28:...|\n",
      "|Mon Apr 06 22:31:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see 10% of date row to see correct timezone before converting\n",
    "tweets_test.select(\"date\").sample(False, 0.1, seed=42).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093c5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 06 22:19:45 PDT 2009\n",
      "Mon Apr 06 22:19:49 PDT 2009\n",
      "Mon Apr 06 22:19:53 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:19:57 PDT 2009\n",
      "Mon Apr 06 22:20:00 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:03 PDT 2009\n",
      "Mon Apr 06 22:20:05 PDT 2009\n",
      "Mon Apr 06 22:20:09 PDT 2009\n"
     ]
    }
   ],
   "source": [
    "# limit to 10 rows of date col.\n",
    "sample_date_values = tweets_test.select(\"date\").limit(10).collect()\n",
    "for row in sample_date_values:\n",
    "    print(row.date)\n",
    "\n",
    "    #with this we can confirm the PDT - Pacific Day Time for apropriate conversion to timestamp.\n",
    "    # this may influence further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d861e2",
   "metadata": {},
   "source": [
    "it's important to account for the PDT timezone used. When converting to datetime, the new schema was in the apropriate datatypes, however when i tried to sample the 'date' rows again i got an error as seen below: \n",
    "\n",
    "> <font color='red'> <b>Py4JJavaError:</b> An error occurred while calling o100.showString.\n",
    ": org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM dd HH:mm:ss z yyyy' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html </font> \n",
    "\n",
    "So according to Apache Spark documentation, I added a date parsing from java with SimpleDateFormat class to allow customization of the date format of the strings. For that the timezone needs to be specified to avoid any discrepancies. In this case, PDT is UTC-7 which is represented by 'z' in the Apache datetime patterns doc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b953588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "tweets_test = tweets_test.withColumn(\"date\", to_timestamp(tweets_test.date, \"EEE MMM dd HH:mm:ss z yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5df066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- query_flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "tweets_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715351a",
   "metadata": {},
   "source": [
    "#### Checking for missing values and shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e25325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----+----+\n",
      "|index|date|query_flag|user|text|\n",
      "+-----+----+----------+----+----+\n",
      "|    0|   0|         0|   0|   0|\n",
      "+-----+----+----------+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:============================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "tweets_test.select([count(when(col(c).isNull(), c)).alias(c) for c in tweets_test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e0e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1600000\n",
      "Number of Columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the sahpe of the dataset\n",
    "num_rows = tweets_test.count()\n",
    "num_cols = len(tweets_test.columns)\n",
    "\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68feb13d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e56df351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|index|               date|query_flag|           user|                text|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "|    0|2009-04-07 05:19:45|  NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|2009-04-07 05:19:49|  NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|2009-04-07 05:19:53|  NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|2009-04-07 05:19:57|  NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|2009-04-07 05:19:57|  NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+-------------------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbcb54",
   "metadata": {},
   "source": [
    "#### Pyspark has some inbuilt functions for starting tne text processing, such as lowercasing, removing special characters and stopwords. \n",
    "\n",
    "The following text data preprocessing steps using PySpark functions are:\n",
    "\n",
    "Lowercasing: We use the lower() function to convert all text to lowercase.\n",
    "\n",
    "Removing Special Characters: We use regexp_replace() to remove any characters that are not alphanumeric or whitespace.\n",
    "\n",
    "Removing Stopwords: We use the StopWordsRemover from the pyspark.ml.feature module to remove common stopwords.\n",
    "\n",
    "The resulting DataFrame tweets_test will have the preprocessed text in the 'text' column.\n",
    "\n",
    "PySpark doesn't have built-in support for stemming or lemmatization. To implement these kind of techniques, external libraries such as nltk can be implemented/imported. NLTK provides a wide range of tools and resources for working with human language data, and it can complement Spark's capabilities in certain scenarios. \n",
    "\n",
    "However, when i tried to import the nltk functions to my vm, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68df2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4 using virtual enviroment 'myenv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dd087",
   "metadata": {},
   "source": [
    "Tokenization is performed using the Tokenizer class.\n",
    "\n",
    "HTML parsing is done using the BeautifulSoup library, and a user-defined function (parse_html_udf) is registered and applied to create a new column named \"cleaned_text.\"\n",
    "\n",
    "A user-defined function is applied to remove special characters and numbers from the \"cleaned_text\" column.\n",
    "Stop words are removed using the StopWordsRemover class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af7d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+---------------+------------------------------------------------------------------------------------------------------------+\n",
      "|index|date               |query_flag|user           |filtered_words                                                                                              |\n",
      "+-----+-------------------+----------+---------------+------------------------------------------------------------------------------------------------------------+\n",
      "|0    |2009-04-07 05:19:45|NO_QUERY  |_TheSpecialOne_|[@switchfoot, http://twitpic.com/2y1zl, -, awww,, bummer., , shoulda, got, david, carr, third, day, it., ;d]|\n",
      "|1    |2009-04-07 05:19:49|NO_QUERY  |scotthamilton  |[upset, update, facebook, texting, it..., might, cry, result, , school, today, also., blah!]                |\n",
      "|2    |2009-04-07 05:19:53|NO_QUERY  |mattycus       |[@kenichan, dived, many, times, ball., managed, save, 50%, , rest, go, bounds]                              |\n",
      "|3    |2009-04-07 05:19:57|NO_QUERY  |ElleCTF        |[whole, body, feels, itchy, like, fire]                                                                     |\n",
      "|4    |2009-04-07 05:19:57|NO_QUERY  |Karoli         |[@nationwideclass, no,, behaving, all., mad., here?, see, there.]                                           |\n",
      "+-----+-------------------+----------+---------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tweets_test = tokenizer.transform(tweets_test)\n",
    "\n",
    "# Define a function for HTML parsing\n",
    "def parse_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Register the function as a UDF\n",
    "parse_html_udf = udf(parse_html, StringType())\n",
    "tweets_test = tweets_test.withColumn(\"cleaned_text\", parse_html_udf(\"text\"))\n",
    "\n",
    "# Remove special characters and numbers\n",
    "tweets_test = tweets_test.withColumn(\"cleaned_text\", udf(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x), StringType())(\"cleaned_text\"))\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "tweets_test = remover.transform(tweets_test).select(\"index\", \"date\", \"query_flag\", \"user\", \"filtered_words\")\n",
    "\n",
    "# Show the result\n",
    "tweets_test.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c00296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# Define a function to remove special characters and punctuation\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Create a UDF (User Defined Function) and apply it to the DataFrame\n",
    "remove_special_chars_udf = udf(remove_special_chars, StringType())\n",
    "df_cleaned = tweets_test.withColumn('cleaned_words', remove_special_chars_udf('filtered_words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13bfbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# Convert text to lowercase\n",
    "df_cleaned = df_cleaned.withColumn('cleaned_words', lower('cleaned_words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8460ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on the 'cleaned_words' column\n",
    "df_cleaned = df_cleaned.dropDuplicates(['cleaned_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39bcc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty 'cleaned_words'\n",
    "df_cleaned = df_cleaned.filter(df_cleaned.cleaned_words != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4e8e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 14:28:38,961 ERROR executor.Executor: Exception in task 2.0 in stage 19.0 (TID 33)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-11-12 14:28:38,993 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 19.0 (TID 33) (192.168.64.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:57)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-11-12 14:28:38,993 ERROR scheduler.TaskSetManager: Task 2 in stage 19.0 failed 1 times; aborting job\n",
      "2023-11-12 14:28:39,093 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 19.0 (TID 31) (192.168.64.6 executor driver): TaskKilled (Stage cancelled)\n",
      "2023-11-12 14:28:39,096 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 19.0 (TID 32) (192.168.64.6 executor driver): TaskKilled (Stage cancelled)\n",
      "2023-11-12 14:28:39,101 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 19.0 (TID 34) (192.168.64.6 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the result\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dec52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e502b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397ba739",
   "metadata": {},
   "source": [
    "- store cleaned dataset back to hbase/mysql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb7845",
   "metadata": {},
   "source": [
    "#### Importing dataset from HBase using a Connector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
